{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/00_pytorch_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","[View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/00_pytorch_fundamentals.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf) | [Watch Video Walkthrough](https://youtu.be/Z_ikDlimN6A?t=76)"]},{"cell_type":"markdown","metadata":{"id":"jSNK7duj5SeU"},"source":["# 00. PyTorch 基础\n","\n","## 什么是 PyTorch？\n","\n","[PyTorch](https://pytorch.org/) 是一个开源的机器学习和深度学习框架。\n","\n","## PyTorch 可以做什么？\n","\n","PyTorch 允许你使用 Python 代码操作和处理数据，编写机器学习算法。\n","\n","## 谁在使用 PyTorch？\n","\n","许多世界上最大的科技公司，如 [Meta (Facebook)](https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/)、特斯拉和微软，以及人工智能研究公司，如 [OpenAI 使用 PyTorch](https://openai.com/blog/openai-pytorch/)，推动研究并将机器学习应用到他们的产品中。\n","\n","![PyTorch 在各个行业和研究中的应用](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-being-used-across-research-and-industry.png)\n","\n","例如，安德烈·卡帕西（特斯拉 AI 负责人）曾多次发表演讲（[PyTorch DevCon 2019](https://youtu.be/oBklltKXtDE)，[特斯拉 AI Day 2021](https://youtu.be/j0z4FweCy4M?t=2904)），讲述特斯拉如何使用 PyTorch 来推动他们的自动驾驶计算机视觉模型。\n","\n","PyTorch 还在其他行业中使用，如农业领域，推动 [拖拉机上的计算机视觉](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)。\n","\n","## 为什么使用 PyTorch？\n","\n","机器学习研究人员喜欢使用 PyTorch。截至 2022 年 2 月，PyTorch 是 [Papers With Code](https://paperswithcode.com/trends) 上使用最广泛的深度学习框架，这个网站用来追踪机器学习研究论文和与之相关的代码库。\n","\n","PyTorch 还帮助处理许多后台任务，比如 GPU 加速（使你的代码运行更快）。\n","\n","这样，你就可以专注于操作数据和编写算法，PyTorch 会确保它运行得很快。\n","\n","如果像特斯拉和 Meta（Facebook）这样的公司使用它来构建他们部署到数百个应用、驱动成千上万的汽车并将内容传递给数十亿人，那么显然它在开发方面也非常强大。\n","\n","## 本模块将涵盖的内容\n","\n","本课程分为不同的章节（笔记本）。\n","\n","每个笔记本都涵盖 PyTorch 中的重要思想和概念。\n","\n","后续的笔记本会在前一个笔记本的基础上进行扩展（编号从 00 开始，依次为 01、02，直到最后）。\n","\n","本笔记本讲解机器学习和深度学习的基本构建块——张量。\n","\n","具体来说，我们将涵盖：\n","\n","| **主题**                   | **内容**                                                                                                                                                                                     |\n","| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n","| **张量介绍**                 | 张量是所有机器学习和深度学习的基本构建块。                                                                                                                                                                      |\n","| **创建张量**                 | 张量几乎可以表示任何类型的数据（图像、词汇、数字表格等）。                                                                                                                                                              |\n","| **从张量中获取信息**             | 如果你能将信息放入张量中，你也希望能将它取出来。                                                                                                                                                                   |\n","| **操作张量**                 | 机器学习算法（如神经网络）涉及以多种方式操作张量，如加法、乘法、组合等。                                                                                                                                                       |\n","| **处理张量形状**               | 机器学习中最常见的问题之一是处理形状不匹配（尝试将错误形状的张量与其他张量混合）。                                                                                                                                                  |\n","| **张量索引**                 | 如果你曾经在 Python 列表或 NumPy 数组中进行索引，张量的索引方式也非常相似，唯一的区别是张量可以有更多维度。                                                                                                                              |\n","| **混合 PyTorch 张量和 NumPy** | PyTorch 玩弄张量（[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html)），NumPy 喜欢数组（[`np.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)），有时你需要将它们混合使用。 |\n","| **可重现性**                 | 机器学习是非常实验性的，由于它使用大量的 *随机性* 来工作，有时你希望这些 *随机性* 不那么随机。                                                                                                                                        |\n","| **在 GPU 上运行张量**          | GPU（图形处理单元）能加快代码的运行速度，PyTorch 使在 GPU 上运行代码变得容易。                                                                                                                                            |\n","\n","## 哪里可以获得帮助？\n","\n","本课程的所有材料都可以在 [GitHub 上找到](https://github.com/mrdbourke/pytorch-deep-learning)。\n","\n","如果遇到问题，你也可以在 [讨论页面](https://github.com/mrdbourke/pytorch-deep-learning/discussions) 提问。\n","\n","还有 [PyTorch 开发者论坛](https://discuss.pytorch.org/)，这是一个非常有帮助的地方，适合所有与 PyTorch 相关的事情。\n"]},{"cell_type":"markdown","metadata":{"id":"5v3iRCRUTGeu"},"source":["## 导入 PyTorch\n","\n","> **注意：** 在运行本笔记本中的任何代码之前，你应该已经完成了 [PyTorch 设置步骤](https://pytorch.org/get-started/locally/)。\n",">\n","> 然而，**如果你在 Google Colab 上运行**，一切应该都能正常工作（Google Colab 已经预装了 PyTorch 和其他库）。\n","\n","让我们首先导入 PyTorch 并检查我们使用的版本。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1VxEOik46Y4i","outputId":"93482d03-5fad-4ac3-bcc2-a465f0721a8d","executionInfo":{"status":"ok","timestamp":1757465014670,"user_tz":-480,"elapsed":3905,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.8.0+cu126'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import torch\n","torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"_SqvI4S9TGew"},"source":["太棒了，看起来我们使用的是 PyTorch 1.10.0+ 版本。\n","\n","这意味着，如果你正在学习这些材料，你会发现大多数内容与 PyTorch 1.10.0+ 版本兼容。但是，如果你的版本号远高于此，可能会遇到一些不一致的情况。\n","\n","如果遇到任何问题，请在课程的 [GitHub 讨论页面](https://github.com/mrdbourke/pytorch-deep-learning/discussions) 发帖。"]},{"cell_type":"markdown","metadata":{"id":"i-33BKR16iWc"},"source":["## 张量介绍\n","\n","现在我们已经导入了 PyTorch，接下来是学习张量的内容。\n","\n","张量是机器学习的基本构建块。\n","\n","它们的作用是以数字方式表示数据。\n","\n","例如，你可以将一张图像表示为形状为 `[3, 224, 224]` 的张量，这意味着 `[颜色通道，高度，宽度]`，也就是说，图像有 `3` 个颜色通道（红色、绿色、蓝色），高度为 `224` 像素，宽度为 `224` 像素。\n","\n","![从输入图像到图像的张量表示的例子，图像被分解为 3 个颜色通道，以及表示高度和宽度的数字](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-tensor-shape-example-of-image.png)\n","\n","在张量术语中（描述张量的语言），该张量有三个维度，分别表示 `颜色通道`、`高度` 和 `宽度`。\n","\n","但我们有些超前了。\n","\n","让我们通过编写代码来更深入地了解张量。\n"]},{"cell_type":"markdown","metadata":{"id":"gFF0N2TU7S7Q"},"source":["### 创建张量\n","\n","PyTorch 非常热爱张量，甚至有一整页文档专门介绍 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) 类。\n","\n","你第一项作业是花 10 分钟阅读 [`torch.Tensor` 的文档](https://pytorch.org/docs/stable/tensors.html)，但你可以稍后再做。\n","\n","现在我们开始编码。\n","\n","我们将首先创建一个 **标量**。\n","\n","标量是一个单一的数字，在张量术语中，它是一个零维张量。\n","\n","> **注意：** 这是本课程的一个趋势。我们将专注于编写具体的代码。但通常我会设置一些涉及阅读并熟悉 PyTorch 文档的练习。毕竟，完成这门课程后，你无疑会想了解更多，而文档是你经常会查阅的地方。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUDgG2zk7Us5","outputId":"c4c020ba-ba2f-47e9-92ea-1b464059935a","executionInfo":{"status":"ok","timestamp":1757465014695,"user_tz":-480,"elapsed":30,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7)"]},"metadata":{},"execution_count":2}],"source":["# Scalar\n","scalar = torch.tensor(7)\n","scalar"]},{"cell_type":"markdown","metadata":{"id":"JqSuhW7rTGey"},"source":["看看上面打印出的 `tensor(7)`？\n","\n","这意味着尽管 `scalar` 是一个单一的数字，但它的类型是 `torch.Tensor`。\n","\n","我们可以使用 `ndim` 属性来检查张量的维度。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lV98Yz868bav","outputId":"0293794c-357c-4dcb-a6ad-806761595eb6","executionInfo":{"status":"ok","timestamp":1757465014708,"user_tz":-480,"elapsed":14,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":3}],"source":["scalar.ndim"]},{"cell_type":"markdown","metadata":{"id":"ZO2YW_QGTGez"},"source":["如果我们想从张量中提取数字呢？\n","\n","也就是说，把它从 `torch.Tensor` 转换为 Python 整数？\n","\n","我们可以使用 `item()` 方法来实现这一点。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-k4cyKumPfbE","outputId":"c1bc5719-1797-4d87-8f9f-ee430f2aba66","executionInfo":{"status":"ok","timestamp":1757465014747,"user_tz":-480,"elapsed":38,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":4}],"source":["# Get the Python number within a tensor (only works with one-element tensors)\n","scalar.item()"]},{"cell_type":"markdown","metadata":{"id":"qYs7ulrATGe0"},"source":["好的，现在让我们看看 **向量**。\n","\n","向量是一个一维张量，但可以包含许多数字。\n","\n","例如，你可以使用向量 `[3, 2]` 来描述你房子的 `[卧室数量, 卫生间数量]`。或者你可以用 `[3, 2, 2]` 来描述 `[卧室数量, 卫生间数量, 车位数量]`。\n","\n","这里的关键点是，向量在表示内容上是灵活的（张量也是如此）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IZF6ASs8QH9","outputId":"5eb4a649-5654-4aeb-ee45-7cec75f63f58","executionInfo":{"status":"ok","timestamp":1757465014760,"user_tz":-480,"elapsed":12,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([7, 7])"]},"metadata":{},"execution_count":5}],"source":["# Vector\n","vector = torch.tensor([7, 7])\n","vector"]},{"cell_type":"markdown","metadata":{"id":"mXxRUUW2TGe1"},"source":["太棒了，`vector` 现在包含了两个 7，这可是我最喜欢的数字。\n","\n","你认为它会有多少个维度呢？\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03hm3VVv8kr4","outputId":"a18cd0f4-0fa1-409d-ecfb-31bd85638dd9","executionInfo":{"status":"ok","timestamp":1757465014829,"user_tz":-480,"elapsed":67,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}],"source":["# Check the number of dimensions of vector\n","vector.ndim"]},{"cell_type":"markdown","metadata":{"id":"W0VYvSGbTGe1"},"source":["嗯，这很奇怪，`vector` 包含两个数字，但只有一个维度。\n","\n","我来告诉你一个小技巧。\n","\n","你可以通过张量外部的方括号数量（`[`）来判断它的维度，并且只需要数一侧。\n","\n","`vector` 有多少个方括号呢？\n","\n","另一个重要的概念是张量的 `shape` 属性。形状告诉你张量内部元素的排列方式。\n","\n","让我们检查一下 `vector` 的形状。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zREV1bDTGe2","outputId":"93251831-9cc4-4913-ebb1-c2a5fec7351f","executionInfo":{"status":"ok","timestamp":1757465014889,"user_tz":-480,"elapsed":59,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2])"]},"metadata":{},"execution_count":7}],"source":["# Check shape of vector\n","vector.shape"]},{"cell_type":"markdown","metadata":{"id":"9aWKppNyTGe2"},"source":["上面的返回值是 `torch.Size([2])`，这意味着我们的向量形状是 `[2]`。这是因为我们在方括号 (`[7, 7]`) 中放入了两个元素。\n","\n","现在，让我们看看 **矩阵**。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5iNwCYL8QO9","outputId":"b2eb44f2-0446-4efa-fb48-f0e3c46916f9","executionInfo":{"status":"ok","timestamp":1757465014923,"user_tz":-480,"elapsed":33,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 7,  8],\n","        [ 9, 10]])"]},"metadata":{},"execution_count":8}],"source":["# Matrix\n","MATRIX = torch.tensor([[7, 8],\n","            [9, 10]])\n","MATRIX"]},{"cell_type":"markdown","metadata":{"id":"a3U1bCdjTGe3"},"source":["哇！更多的数字！矩阵和向量一样灵活，只不过它们多了一个维度。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LREUbeb8r8j","outputId":"bfbc191a-6db2-4508-9f47-ab26cd714c36","executionInfo":{"status":"ok","timestamp":1757465014979,"user_tz":-480,"elapsed":55,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":9}],"source":["# Check number of dimensions\n","MATRIX.ndim"]},{"cell_type":"markdown","metadata":{"id":"LhXXgq-dTGe3"},"source":["`MATRIX` 有两个维度（你数过外部方括号的数量吗？）。\n","\n","你觉得它的 `shape` 会是什么样的呢？\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TL26I31TGe3","outputId":"9e39f9a0-b85f-4f40-9aea-682903e9a347","executionInfo":{"status":"ok","timestamp":1757465015005,"user_tz":-480,"elapsed":25,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":10}],"source":["MATRIX.shape"]},{"cell_type":"markdown","metadata":{"id":"dvLpUvrKTGe4"},"source":["我们得到的输出是 `torch.Size([2, 2])`，因为 `MATRIX` 在深度和宽度上各有两个元素。\n","\n","接下来我们来创建一个 **张量** 吧！\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEMDQr188QWW","outputId":"caa6df35-80b2-435a-a92c-cc89c9d1efa2","executionInfo":{"status":"ok","timestamp":1757465015013,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1, 2, 3],\n","         [3, 6, 9],\n","         [2, 4, 5]]])"]},"metadata":{},"execution_count":11}],"source":["# Tensor\n","TENSOR = torch.tensor([[[1, 2, 3],\n","            [3, 6, 9],\n","            [2, 4, 5]]])\n","TENSOR"]},{"cell_type":"markdown","metadata":{"id":"UmJKkXD7TGe4"},"source":["哇！这个张量看起来真不错。\n","\n","我想强调的是，张量几乎可以表示任何东西。\n","\n","我们刚刚创建的这个张量可能代表了一个牛排和杏仁黄油店的销售数据（这是我最喜欢的两种食物）。\n","\n","![Google Sheets 中的简单张量，显示了星期几、牛排销售和杏仁黄油销售](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_simple_tensor.png)\n","\n","你认为它有多少个维度？（提示：使用方括号计数的技巧）\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dhuEsjS8QcT","outputId":"223ddd4d-44db-40da-e58a-9af16bf4c33b","executionInfo":{"status":"ok","timestamp":1757465015021,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":12}],"source":["# Check number of dimensions for TENSOR\n","TENSOR.ndim"]},{"cell_type":"markdown","metadata":{"id":"ln9dys5VTGe4"},"source":["它的`shape`是什么呢？"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdVv4iNRTGe5","outputId":"1562caf5-765b-4838-a535-ae5292bd4c7d","executionInfo":{"status":"ok","timestamp":1757465015091,"user_tz":-480,"elapsed":70,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 3])"]},"metadata":{},"execution_count":13}],"source":["# Check shape of TENSOR\n","TENSOR.shape"]},{"cell_type":"markdown","metadata":{"id":"zxk8GU7oTGe5"},"source":["好，输出是 `torch.Size([1, 3, 3])`。\n","\n","维度从外到内排列。\n","\n","这意味着它有一个 3x3 的维度。\n","\n","![不同张量维度的示例](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n","\n","> **注意：** 你可能注意到我在 `scalar` 和 `vector` 上使用了小写字母，而在 `MATRIX` 和 `TENSOR` 上使用了大写字母。这是故意的。实际上，你通常会看到标量和向量用小写字母表示，如 `y` 或 `a`；而矩阵和张量用大写字母表示，如 `X` 或 `W`。\n",">\n","> 你还可能注意到，矩阵和张量的名称常常可以互换使用。这是常见的。在 PyTorch 中，你通常会处理 `torch.Tensor`（因此有了张量这个名称），但其中内容的形状和维度将决定它实际是什么。\n","\n","让我们总结一下。\n","\n","| 名称     | 它是什么？                           | 维度数                       | 小写还是大写（通常/示例） |\n","| ------ | ------------------------------- | ------------------------- | ------------- |\n","| **标量** | 一个单一的数字                         | 0                         | 小写 (`a`)      |\n","| **向量** | 带有方向的数字（例如，带方向的风速），但也可以包含其他多个数字 | 1                         | 小写 (`y`)      |\n","| **矩阵** | 一个二维的数字数组                       | 2                         | 大写 (`Q`)      |\n","| **张量** | 一个 n 维的数字数组                     | 可以是任何维度，0 维张量是标量，1 维张量是向量 | 大写 (`X`)      |\n","\n","![标量、向量、矩阵、张量及其外观](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)\n"]},{"cell_type":"markdown","metadata":{"id":"dms7G4nkTGe5"},"source":["### 随机张量\n","\n","我们已经确定，张量表示某种形式的数据。\n","\n","机器学习模型，如神经网络，操作张量并在其中寻找模式。\n","\n","但是在使用 PyTorch 构建机器学习模型时，你很少会手动创建张量（像我们之前所做的那样）。\n","\n","相反，机器学习模型通常从大量随机的数字张量开始，并在处理数据时调整这些随机数字，以更好地表示数据。\n","\n","本质上：\n","\n","`从随机数字开始 -> 查看数据 -> 更新随机数字 -> 查看数据 -> 更新随机数字...`\n","\n","作为数据科学家，你可以定义机器学习模型的起始方式（初始化）、如何查看数据（表示）以及如何更新（优化）它的随机数字。\n","\n","我们稍后会动手实践这些步骤。\n","\n","现在，让我们看看如何创建一个随机数字的张量。\n","\n","我们可以使用 [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html) 并传入 `size` 参数来实现。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOJEtDx--GnK","outputId":"eb2d249f-3ef3-417c-938c-ce4bd1f7608a","executionInfo":{"status":"ok","timestamp":1757465015117,"user_tz":-480,"elapsed":25,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.3752, 0.0470, 0.8766, 0.4243],\n","         [0.8861, 0.2201, 0.3930, 0.6026],\n","         [0.3849, 0.7988, 0.2604, 0.1869]]),\n"," torch.float32)"]},"metadata":{},"execution_count":14}],"source":["# Create a random tensor of size (3, 4)\n","random_tensor = torch.rand(size=(3, 4))\n","random_tensor, random_tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"-wB1c_cXTGe5"},"source":["`torch.rand()` 的灵活性在于，我们可以根据需要调整 `size`。\n","\n","例如，假设你想要一个常见图像形状的随机张量 `[224, 224, 3]`（`[高度, 宽度, 颜色通道]`）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMF_NUp3Ym__","outputId":"1e3b0152-118f-4a09-b617-ea5cb63219e1","executionInfo":{"status":"ok","timestamp":1757465015193,"user_tz":-480,"elapsed":75,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([224, 224, 3]), 3)"]},"metadata":{},"execution_count":15}],"source":["# Create a random tensor of size (224, 224, 3)\n","random_image_size_tensor = torch.rand(size=(224, 224, 3))\n","random_image_size_tensor.shape, random_image_size_tensor.ndim"]},{"cell_type":"markdown","metadata":{"id":"0MQNTY0eTGe6"},"source":["### Zeros and ones\n","\n","有时，你只想用零或一填充张量。\n","\n","这在很多情况下都很常见，尤其是在掩码操作中（例如，用零掩盖张量中的某些值，让模型知道不要学习这些值）。\n","\n","让我们使用 [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html) 创建一个充满零的张量。\n","\n","同样，`size` 参数将发挥作用。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCzhd0hl9Vp6","outputId":"bdab069e-db84-4253-86aa-2d91d9275d0f","executionInfo":{"status":"ok","timestamp":1757465015194,"user_tz":-480,"elapsed":55,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]),\n"," torch.float32)"]},"metadata":{},"execution_count":16}],"source":["# Create a tensor of all zeros\n","zeros = torch.zeros(size=(3, 4))\n","zeros, zeros.dtype"]},{"cell_type":"markdown","metadata":{"id":"WDQBZJRUZWTN"},"source":["我们可以做同样的事情，使用 [`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html) 来创建一个充满一的张量。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRe6sSXiTGe6","outputId":"547e365d-08ce-46d7-bba9-665b6044fbb3","executionInfo":{"status":"ok","timestamp":1757465015194,"user_tz":-480,"elapsed":48,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]),\n"," torch.float32)"]},"metadata":{},"execution_count":17}],"source":["# Create a tensor of all ones\n","ones = torch.ones(size=(3, 4))\n","ones, ones.dtype"]},{"cell_type":"markdown","metadata":{"id":"hib1NYrSarL2"},"source":["### 创建范围和类似张量\n","\n","有时你可能想要一系列的数字，比如从 1 到 10 或从 0 到 100。\n","\n","你可以使用 `torch.arange(start, end, step)` 来实现。\n","\n","其中：\n","\n","* `start` = 范围的起始值（例如 0）\n","* `end` = 范围的结束值（例如 10）\n","* `step` = 每个值之间的步长（例如 1）\n","\n","> **注意：** 在 Python 中，你可以使用 `range()` 来创建范围。但在 PyTorch 中，`torch.range()` 已被弃用，未来可能会报错。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IqUs81d9W4W","outputId":"7dd5c603-a211-403d-ddc3-7a4fdd9cd684","executionInfo":{"status":"ok","timestamp":1757465015194,"user_tz":-480,"elapsed":40,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1814108326.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{},"execution_count":18}],"source":["# Use torch.arange(), torch.range() is deprecated\n","zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n","\n","# Create a range of values 0 to 10\n","zero_to_ten = torch.arange(start=0, end=10, step=1)\n","zero_to_ten"]},{"cell_type":"markdown","metadata":{"id":"i-bXf0Ugbh-D"},"source":["有时你可能想要一个与另一个张量具有相同形状的张量。\n","\n","例如，一个与之前的张量形状相同的全零张量。\n","\n","为此，你可以使用 [`torch.zeros_like(input)`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) 或 [`torch.ones_like(input)`](https://pytorch.org/docs/1.9.1/generated/torch.ones_like.html)，它们分别返回一个与 `input` 形状相同、填充零或一的张量。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvXwUut5BhHq","outputId":"f1471678-8fb3-4afc-ae38-abb0083273df","executionInfo":{"status":"ok","timestamp":1757465015201,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"metadata":{},"execution_count":19}],"source":["# Can also create a tensor of zeros similar to another tensor\n","ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n","ten_zeros"]},{"cell_type":"markdown","metadata":{"id":"huKZ6QlYTGe7"},"source":["### 张量数据类型\n","\n","PyTorch 中有许多不同的 [张量数据类型](https://pytorch.org/docs/stable/tensors.html#data-types)。\n","\n","有些特定于 CPU，有些则更适用于 GPU。\n","\n","了解每种数据类型可能需要一些时间。\n","\n","通常，如果你看到 `torch.cuda`，那么张量是用于 GPU 的（因为 Nvidia 的 GPU 使用名为 CUDA 的计算工具包）。\n","\n","最常见的类型（通常也是默认类型）是 `torch.float32` 或 `torch.float`。\n","\n","这被称为 \"32 位浮动点\"。\n","\n","但也有 16 位浮动点（`torch.float16` 或 `torch.half`）和 64 位浮动点（`torch.float64` 或 `torch.double`）。\n","\n","为了让事情更复杂一些，还有 8 位、16 位、32 位和 64 位的整数。\n","\n","以及更多！\n","\n","> **注意：** 整数是像 `7` 这样的平整整数，而浮动点数是带小数点的 `7.0`。\n","\n","所有这些数据类型的原因都与 **计算中的精度** 有关。\n","\n","精度是用来描述一个数字的详细程度。\n","\n","精度值越高（8、16、32），表示描述数字时使用的细节和数据越多。\n","\n","这在深度学习和数值计算中非常重要，因为你需要进行许多操作，计算时所需的细节越多，计算资源的消耗也越大。\n","\n","因此，低精度数据类型通常计算更快，但会牺牲一些评估指标的性能，比如准确度（计算更快，但准确度较低）。\n","\n","> **资源：**\n","\n","* 请查看 [PyTorch 文档中所有可用的张量数据类型](https://pytorch.org/docs/stable/tensors.html#data-types)。\n","* 阅读 [Wikipedia 页面，了解计算中的精度](https://en.wikipedia.org/wiki/Precision_%28computer_science%29)。\n","\n","让我们看看如何创建具有特定数据类型的张量。我们可以使用 `dtype` 参数来实现。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3MoGnpw9XaF","outputId":"e6d576f6-2f72-4d2c-d680-43acb65bd577","executionInfo":{"status":"ok","timestamp":1757465015209,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3]), torch.float32, device(type='cpu'))"]},"metadata":{},"execution_count":20}],"source":["# Default datatype for tensors is float32\n","float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n","                dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n","                device=None, # defaults to None, which uses the default tensor type\n","                requires_grad=False) # if True, operations performed on the tensor are recorded\n","\n","float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"]},{"cell_type":"markdown","metadata":{"id":"MhP8kzDfe_ty"},"source":["除了形状问题（张量形状不匹配），在 PyTorch 中你会遇到的另两个最常见的问题是数据类型和设备问题。\n","\n","例如，一个张量是 `torch.float32`，另一个是 `torch.float16`（PyTorch 通常希望张量使用相同的格式）。\n","\n","或者一个张量在 CPU 上，另一个在 GPU 上（PyTorch 喜欢在同一设备上进行张量之间的计算）。\n","\n","我们稍后会深入讨论设备问题。\n","\n","现在，让我们创建一个 `dtype=torch.float16` 的张量。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKSuajld_09s","outputId":"399c5c05-e3a4-40ed-bfb5-ab870a3bb8cd","executionInfo":{"status":"ok","timestamp":1757465015215,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float16"]},"metadata":{},"execution_count":21}],"source":["float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n","          dtype=torch.float16) # torch.half would also work\n","\n","float_16_tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"gUjkB2AX7Upz"},"source":["## 从张量获取信息\n","\n","一旦你创建了张量（或者是别人或 PyTorch 模块为你创建的），你可能会想要获取一些关于它的信息。\n","\n","我们之前已经看到过，但最常见的三个张量属性是：\n","\n","* `shape` - 张量的形状是什么？（有些操作要求特定的形状规则）\n","* `dtype` - 张量中的元素存储在哪种数据类型中？\n","* `device` - 张量存储在哪个设备上？（通常是 GPU 或 CPU）\n","\n","让我们创建一个随机张量并获取它的详细信息。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hd_X4D0j7Umq","outputId":"403ffbba-73fc-4eea-ed2b-4f88aed79dcf","executionInfo":{"status":"ok","timestamp":1757465015246,"user_tz":-480,"elapsed":30,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1226, 0.9552, 0.6248, 0.7139],\n","        [0.4162, 0.9144, 0.5538, 0.2994],\n","        [0.1326, 0.4641, 0.4898, 0.8354]])\n","Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["# Create a tensor\n","some_tensor = torch.rand(3, 4)\n","\n","# Find out details about it\n","print(some_tensor)\n","print(f\"Shape of tensor: {some_tensor.shape}\")\n","print(f\"Datatype of tensor: {some_tensor.dtype}\")\n","print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU"]},{"cell_type":"markdown","metadata":{"id":"45K-E5uPg6cj"},"source":["> **注意：** 当你在 PyTorch 中遇到问题时，通常与上面提到的三个属性之一有关。因此，当错误信息出现时，给自己唱一首小歌叫做“什么，什么，哪里”：\n","\n","* \"*我的张量是什么形状？它们的数据类型是什么，存储在哪里？什么形状，什么数据类型，在哪里在哪里*”。\n"]},{"cell_type":"markdown","metadata":{"id":"BdiWvoAi7UjL"},"source":["## 操作张量（张量运算）\n","\n","在深度学习中，数据（图像、文本、视频、音频、蛋白质结构等）被表示为张量。\n","\n","模型通过调查这些张量并对张量执行一系列操作（可能是数百万次以上），来创建输入数据中模式的表示。\n","\n","这些操作通常是：\n","\n","* 加法\n","* 减法\n","* 乘法（逐元素）\n","* 除法\n","* 矩阵乘法\n","\n","就这些了。确实这里还有一些其他操作，但这些是神经网络的基本构建块。\n","\n","将这些构建块以正确的方式堆叠起来，你就可以创建最复杂的神经网络（就像拼乐高一样！）。\n"]},{"cell_type":"markdown","metadata":{"id":"Sk_6Dd7L7Uce"},"source":["### 基本运算\n","\n","让我们从一些基本的运算开始：加法（`+`）、减法（`-`）、乘法（`*`）。\n","\n","它们的运作方式和你想的完全一样。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X71WpQoPD7a4","outputId":"5748a6a5-599d-4b5c-fadc-16001db09cb5","executionInfo":{"status":"ok","timestamp":1757465015253,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([11, 12, 13])"]},"metadata":{},"execution_count":23}],"source":["# Create a tensor of values and add a number to it\n","tensor = torch.tensor([1, 2, 3])\n","tensor + 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp4TlTWWEFeO","outputId":"abfd5110-bf8a-48d3-9e78-dbe6247080c0","executionInfo":{"status":"ok","timestamp":1757465015275,"user_tz":-480,"elapsed":22,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30])"]},"metadata":{},"execution_count":24}],"source":["# Multiply it by 10\n","tensor * 10"]},{"cell_type":"markdown","metadata":{"id":"-1VEHnuRkn8Q"},"source":["注意到上面的张量值没有变成 `tensor([110, 120, 130])` 吗？这是因为张量中的值不会自动改变，除非它们被重新赋值。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuB1UjCIEJIA","outputId":"cd79ec96-81d7-445b-86bf-e2f08eb81a46","executionInfo":{"status":"ok","timestamp":1757465015287,"user_tz":-480,"elapsed":21,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":25}],"source":["# Tensors don't change unless reassigned\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"VYvqGpUTk1o6"},"source":["让我们减去一个数字，这次我们会重新赋值给 `tensor` 变量。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4iWKoLsENry","outputId":"c6abb130-3075-467b-86ff-70e47471de9f","executionInfo":{"status":"ok","timestamp":1757465015309,"user_tz":-480,"elapsed":10,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-9, -8, -7])"]},"metadata":{},"execution_count":26}],"source":["# Subtract and reassign\n","tensor = tensor - 10\n","tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFgZY-PaFNXa","outputId":"7269047e-a710-4f27-b398-cf60cada44c9","executionInfo":{"status":"ok","timestamp":1757465015342,"user_tz":-480,"elapsed":33,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":27}],"source":["# Add and reassign\n","tensor = tensor + 10\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"CYXDoIOzk-6I"},"source":["PyTorch 还提供了许多内置函数，如 [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul)（乘法的简写）和 [`torch.add()`](https://pytorch.org/docs/stable/generated/torch.add.html) 来执行基本运算。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVysdk3kFWbY","outputId":"eac72087-951a-44c2-9775-592f40cc1d89","executionInfo":{"status":"ok","timestamp":1757465015353,"user_tz":-480,"elapsed":10,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30])"]},"metadata":{},"execution_count":28}],"source":["# Can also use torch functions\n","torch.multiply(tensor, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxuPJIpNFbqO","outputId":"deb9e81d-a3d5-4b7c-e6f6-898ca00b9e12","executionInfo":{"status":"ok","timestamp":1757465015360,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":29}],"source":["# Original tensor is still unchanged\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"70UNL33AlVQq"},"source":["然而，更常见的做法是使用操作符符号，如 `*`，而不是 `torch.mul()`。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5v3RkR0F2Jq","outputId":"cf94463e-b5b1-46ba-d518-2c0d7172c50c","executionInfo":{"status":"ok","timestamp":1757465015390,"user_tz":-480,"elapsed":29,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) * tensor([1, 2, 3])\n","Equals: tensor([1, 4, 9])\n"]}],"source":["# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n","print(tensor, \"*\", tensor)\n","print(\"Equals:\", tensor * tensor)"]},{"cell_type":"markdown","metadata":{"id":"TT5fVuyu7q5z"},"source":["### 矩阵乘法（就是你需要的）\n","\n","机器学习和深度学习算法（如神经网络）中最常见的运算之一是 [矩阵乘法](https://www.mathsisfun.com/algebra/matrix-multiplying.html)。\n","\n","PyTorch 在 [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) 方法中实现了矩阵乘法功能。\n","\n","记住矩阵乘法的两个主要规则：\n","\n","1. **内维度**必须匹配：\n","\n","* `(3, 2) @ (3, 2)` 不行\n","* `(2, 3) @ (3, 2)` 可以\n","* `(3, 2) @ (2, 3)` 可以\n","\n","2. 结果矩阵的形状是**外维度**：\n","\n","* `(2, 3) @ (3, 2)` -> `(2, 2)`\n","* `(3, 2) @ (2, 3)` -> `(3, 3)`\n","\n","> **注意：** 在 Python 中，“`@`” 是矩阵乘法的符号。\n","\n","> **资源：** 你可以在 PyTorch 文档中查看使用 `torch.matmul()` 的所有矩阵乘法规则：[PyTorch 文档](https://pytorch.org/docs/stable/generated/torch.matmul.html)。\n","\n","让我们创建一个张量，并对其进行逐元素乘法和矩阵乘法。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZE7loucmDlEM","outputId":"a3c21297-0841-4810-a70e-c4526b068571","executionInfo":{"status":"ok","timestamp":1757465015391,"user_tz":-480,"elapsed":23,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3])"]},"metadata":{},"execution_count":31}],"source":["import torch\n","tensor = torch.tensor([1, 2, 3])\n","tensor.shape"]},{"cell_type":"markdown","metadata":{"id":"VUAZ3_b0vOKv"},"source":["逐元素乘法和矩阵乘法的区别在于是否有值的加法。\n","\n","对于我们的 `tensor` 变量，其值为 `[1, 2, 3]`：\n","\n","| 操作        | 计算                              | 代码                      |\n","| --------- | ------------------------------- | ----------------------- |\n","| **逐元素乘法** | `[1*1, 2*2, 3*3]` = `[1, 4, 9]` | `tensor * tensor`       |\n","| **矩阵乘法**  | `[1*1 + 2*2 + 3*3]` = `[14]`    | `tensor.matmul(tensor)` |\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i42gkUeHvI_1","outputId":"2362a050-23e1-4490-9cc3-8ca06c2fa556","executionInfo":{"status":"ok","timestamp":1757465015392,"user_tz":-480,"elapsed":15,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 4, 9])"]},"metadata":{},"execution_count":32}],"source":["# Element-wise matrix multiplication\n","tensor * tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvCBiiTTDk8y","outputId":"1a9c8937-0bbc-4cb9-9b6b-5e1e8d338443","executionInfo":{"status":"ok","timestamp":1757465015392,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":33}],"source":["# Matrix multiplication\n","torch.matmul(tensor, tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4E_pROBDk2r","outputId":"1210b561-f85c-41be-ea3b-4ba9eb767345","executionInfo":{"status":"ok","timestamp":1757465015403,"user_tz":-480,"elapsed":10,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":34}],"source":["# Can also use the \"@\" symbol for matrix multiplication, though not recommended\n","tensor @ tensor"]},{"cell_type":"markdown","metadata":{"id":"obbginUMv43A"},"source":["你可以手动进行矩阵乘法，但不推荐这样做。\n","\n","内置的 `torch.matmul()` 方法更快。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qMSaLOoJscL","outputId":"855dbec7-a756-4005-806e-e5a756540006","executionInfo":{"status":"ok","timestamp":1757465015411,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 272 µs, sys: 884 µs, total: 1.16 ms\n","Wall time: 2.55 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":35}],"source":["%%time\n","# Matrix multiplication by hand\n","# (avoid doing operations with for loops at all cost, they are computationally expensive)\n","value = 0\n","for i in range(len(tensor)):\n","  value += tensor[i] * tensor[i]\n","value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVWiKB0KwH74","outputId":"6e795ec4-1fc8-4c81-acca-ac7de1f8d761","executionInfo":{"status":"ok","timestamp":1757465015418,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 0 ns, sys: 210 µs, total: 210 µs\n","Wall time: 155 µs\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":36}],"source":["%%time\n","torch.matmul(tensor, tensor)"]},{"cell_type":"markdown","metadata":{"id":"aJ4DDmo1TGe-"},"source":["## 深度学习中最常见的错误之一（形状错误）\n","\n","由于深度学习的大部分工作都涉及矩阵的乘法和操作，而矩阵在组合时有严格的形状和尺寸规则，因此你在深度学习中最常遇到的错误之一就是形状不匹配。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"rN5RcoD4Jo6y","outputId":"9ed5042d-912b-476e-b4b4-f17883f02301","executionInfo":{"status":"error","timestamp":1757465015428,"user_tz":-480,"elapsed":9,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2051701859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               [9, 12]], dtype=torch.float32)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"]}],"source":["# Shapes need to be in the right way\n","tensor_A = torch.tensor([[1, 2],\n","            [3, 4],\n","            [5, 6]], dtype=torch.float32)\n","\n","tensor_B = torch.tensor([[7, 10],\n","              [8, 11],\n","              [9, 12]], dtype=torch.float32)\n","\n","torch.matmul(tensor_A, tensor_B) # (this will error)"]},{"cell_type":"markdown","metadata":{"id":"HNA6MZEFxWVt"},"source":["我们可以通过使 `tensor_A` 和 `tensor_B` 的内维度匹配来使矩阵乘法正常工作。\n","\n","实现这一点的方法之一是使用 **转置**（交换给定张量的维度）。\n","\n","你可以在 PyTorch 中使用以下两种方式进行转置：\n","\n","* `torch.transpose(input, dim0, dim1)` - 其中 `input` 是要转置的张量，`dim0` 和 `dim1` 是要交换的维度。\n","* `tensor.T` - 其中 `tensor` 是要转置的张量。\n","\n","让我们尝试后者。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUqgaANiy1wq","outputId":"d7f278bc-19be-425f-b6bc-5932e11b0d4f","executionInfo":{"status":"ok","timestamp":1757465020901,"user_tz":-480,"elapsed":9,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7., 10.],\n","        [ 8., 11.],\n","        [ 9., 12.]])\n"]}],"source":["# View tensor_A and tensor_B\n","print(tensor_A)\n","print(tensor_B)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DveqxO7iy_Fi","outputId":"db0b2e43-5f25-4438-adea-3057e140adaa","executionInfo":{"status":"ok","timestamp":1757465023314,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7.,  8.,  9.],\n","        [10., 11., 12.]])\n"]}],"source":["# View tensor_A and tensor_B.T\n","print(tensor_A)\n","print(tensor_B.T)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35rEIu-NKtVE","outputId":"e811fd17-7ad4-4c1f-922a-51dfb0edef83","executionInfo":{"status":"ok","timestamp":1757465029508,"user_tz":-480,"elapsed":52,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n","\n","New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n","\n","Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n","\n","Output:\n","\n","tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])\n","\n","Output shape: torch.Size([3, 3])\n"]}],"source":["# The operation works when tensor_B is transposed\n","print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n","print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n","print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n","print(\"Output:\\n\")\n","output = torch.matmul(tensor_A, tensor_B.T)\n","print(output)\n","print(f\"\\nOutput shape: {output.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"MfcFEqfLjN24"},"source":["你也可以使用 [`torch.mm()`](https://pytorch.org/docs/stable/generated/torch.mm.html)，它是 `torch.matmul()` 的简写。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3rJvW_TTGe_","outputId":"7ea57b9b-811c-4fcc-ba17-9d45b8b6c200","executionInfo":{"status":"ok","timestamp":1757465033407,"user_tz":-480,"elapsed":27,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])"]},"metadata":{},"execution_count":41}],"source":["# torch.mm is a shortcut for matmul\n","torch.mm(tensor_A, tensor_B.T)"]},{"cell_type":"markdown","metadata":{"id":"bXKozI4T0hFi"},"source":["如果没有转置，矩阵乘法的规则就无法满足，我们会遇到像上面那样的错误。\n","\n","来一个可视化示例吧！\n","\n","![矩阵乘法可视化演示](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/00-matrix-multiply-crop.gif)\n","\n","你可以通过访问 [http://matrixmultiplication.xyz/](http://matrixmultiplication.xyz/) 来创建自己的矩阵乘法可视化。\n","\n","> **注意：** 这样的矩阵乘法也被称为两个矩阵的 [**点积**](https://www.mathsisfun.com/algebra/vectors-dot-product.html)。\n"]},{"cell_type":"markdown","metadata":{"id":"hA64Z4DmkB31"},"source":["神经网络充满了矩阵乘法和点积。\n","\n","[`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) 模块（我们稍后会看到它的实际应用），也被称为前馈层或全连接层，实现了输入 `x` 和权重矩阵 `A` 之间的矩阵乘法。\n","\n","$$\n","y = x\\cdot{A^T} + b\n","$$\n","\n","其中：\n","\n","* `x` 是该层的输入（深度学习是由像 `torch.nn.Linear()` 这样的层堆叠在一起构成的）。\n","* `A` 是由该层创建的权重矩阵，最开始是随机数，在神经网络学习过程中进行调整，以更好地表示数据中的模式（注意 \"`T`\" 表示权重矩阵需要转置）。\n","\n","  * **注意：** 你也常常会看到 `W` 或其他字母（如 `X`）来表示权重矩阵。\n","* `b` 是偏置项，用于稍微调整权重和输入。\n","* `y` 是输出（对输入的处理，目的是发现其中的模式）。\n","\n","这是一个线性函数（你可能在高中或其他地方见过类似 \\$y = mx + b\\$ 的公式），并且可以用来画出一条直线！\n","\n","让我们来玩一下线性层。\n","\n","尝试改变下面 `in_features` 和 `out_features` 的值，看看会发生什么。\n","\n","你注意到关于形状的变化吗？\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mC_MjKW1LX7T","outputId":"ec2a912f-42bd-432d-9b53-f33a1f339e85","executionInfo":{"status":"ok","timestamp":1757465036080,"user_tz":-480,"elapsed":53,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([3, 2])\n","\n","Output:\n","tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n","        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n","        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n","       grad_fn=<AddmmBackward0>)\n","\n","Output shape: torch.Size([3, 6])\n"]}],"source":["# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n","torch.manual_seed(42)\n","# This uses matrix multiplication\n","linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input\n","              out_features=6) # out_features = describes outer value\n","x = tensor_A\n","output = linear(x)\n","print(f\"Input shape: {x.shape}\\n\")\n","print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"zIGrP5j1pN7j"},"source":["> **问题：** 如果你将上面的 `in_features` 从 2 改为 3 会发生什么？会出错吗？如何更改输入 (`x`) 的形状以适应错误？提示：我们之前对 `tensor_B` 做了什么？\n"]},{"cell_type":"markdown","metadata":{"id":"EPNF0nMWoGEj"},"source":["如果你以前从未接触过，矩阵乘法一开始可能会让人感到困惑。\n","\n","但在你玩弄它几次之后，甚至打开几个神经网络，你会发现它无处不在。\n","\n","记住，矩阵乘法就是你所需要的。\n","\n","![矩阵乘法就是你所需要的](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_matrix_multiplication_is_all_you_need.jpeg)\n","\n","*当你开始深入了解神经网络层并构建自己的神经网络时，你会发现矩阵乘法无处不在。**来源：** [Working Class Deep Learner](https://marksaroufim.substack.com/p/working-class-deep-learner)*\n"]},{"cell_type":"markdown","metadata":{"id":"pjMmrJOOPv5e"},"source":["### 查找最小值、最大值、均值、总和等（聚合）\n","\n","现在我们已经看过一些操作张量的方法，让我们快速了解几种聚合方法（从更多的值变成更少的值）。\n","\n","首先，我们将创建一个张量，然后找出它的最大值、最小值、均值和总和。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrFQbe5fP1Rk","outputId":"855fcce0-e68b-46b6-fb49-8fb73fc7530a","executionInfo":{"status":"ok","timestamp":1757465039257,"user_tz":-480,"elapsed":14,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"]},"metadata":{},"execution_count":43}],"source":["# Create a tensor\n","x = torch.arange(0, 100, 10)\n","x"]},{"cell_type":"markdown","metadata":{"id":"-J-wfMdlsEco"},"source":["现在让我们进行一些聚合操作。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5wSP9YKP3Lb","outputId":"87af7df9-b6ab-45c6-84be-6479097d0b35","executionInfo":{"status":"ok","timestamp":1757465043360,"user_tz":-480,"elapsed":18,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum: 0\n","Maximum: 90\n","Mean: 45.0\n","Sum: 450\n"]}],"source":["print(f\"Minimum: {x.min()}\")\n","print(f\"Maximum: {x.max()}\")\n","# print(f\"Mean: {x.mean()}\") # this will error\n","print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n","print(f\"Sum: {x.sum()}\")"]},{"cell_type":"markdown","metadata":{"id":"JHoKpsg3sKQE"},"source":["> **注意：** 你可能会发现一些方法，如 `torch.mean()`，要求张量必须是 `torch.float32`（最常见的）或其他特定的数据类型，否则操作将失败。\n","\n","你也可以使用 `torch` 方法做和上面一样的操作。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Cr23Y9uP3HO","outputId":"310f85f2-3197-41eb-d96a-a45898c63b62","executionInfo":{"status":"ok","timestamp":1757465045215,"user_tz":-480,"elapsed":92,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(90), tensor(0), tensor(45.), tensor(450))"]},"metadata":{},"execution_count":45}],"source":["torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"]},{"cell_type":"markdown","metadata":{"id":"i7ApCaZjDkvp"},"source":["### 位置最小值/最大值\n","\n","你也可以使用 [`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) 和 [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html) 分别找到张量中最大值或最小值的索引。\n","\n","这在你只需要知道最高（或最低）值的位置，而不是实际值时非常有用（我们将在后面的章节中看到，当使用 [softmax 激活函数](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) 时）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzNBl9JSGlHi","outputId":"ed41feeb-7071-4f08-fa1d-618e8b74bc7a","executionInfo":{"status":"ok","timestamp":1757465047573,"user_tz":-480,"elapsed":26,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n","Index where max value occurs: 8\n","Index where min value occurs: 0\n"]}],"source":["# Create a tensor\n","tensor = torch.arange(10, 100, 10)\n","print(f\"Tensor: {tensor}\")\n","\n","# Returns index of max and min values\n","print(f\"Index where max value occurs: {tensor.argmax()}\")\n","print(f\"Index where min value occurs: {tensor.argmin()}\")"]},{"cell_type":"markdown","metadata":{"id":"QBu33WihOXBk"},"source":["### 更改张量数据类型\n","\n","如前所述，深度学习操作中常见的问题之一是张量的数据类型不同。\n","\n","如果一个张量是 `torch.float64`，而另一个是 `torch.float32`，你可能会遇到一些错误。\n","\n","但这是可以解决的。\n","\n","你可以使用 [`torch.Tensor.type(dtype=None)`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html) 来更改张量的数据类型，其中 `dtype` 参数是你想使用的数据类型。\n","\n","首先，我们将创建一个张量并检查它的数据类型（默认是 `torch.float32`）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rY2FEsCAOaLu","outputId":"6fbe9a27-5a9b-46d8-b8ed-3393d4b88ed1","executionInfo":{"status":"ok","timestamp":1757465120188,"user_tz":-480,"elapsed":14,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":49}],"source":["# Create a tensor and check its datatype\n","tensor = torch.arange(10., 100., 10.)\n","tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"jR30FHEc92of"},"source":["现在我们将创建另一个张量，与之前相同，但将它的数据类型更改为 `torch.float16`。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cac8gRYjOeab","outputId":"bd2a0de0-336a-4484-c18e-2105b450f15f","executionInfo":{"status":"ok","timestamp":1757465130062,"user_tz":-480,"elapsed":19,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"]},"metadata":{},"execution_count":50}],"source":["# Create a float16 tensor\n","tensor_float16 = tensor.type(torch.float16)\n","tensor_float16"]},{"cell_type":"markdown","metadata":{"id":"ndVlKJZ4-7_5"},"source":["我们可以做类似的操作来创建一个 `torch.int8` 类型的张量。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Yqovld2Oj6s","outputId":"0b54d507-7876-41fc-943c-686bedf2dd03","executionInfo":{"status":"ok","timestamp":1757465133893,"user_tz":-480,"elapsed":23,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"]},"metadata":{},"execution_count":51}],"source":["# Create an int8 tensor\n","tensor_int8 = tensor.type(torch.int8)\n","tensor_int8"]},{"cell_type":"markdown","metadata":{"id":"44GxVabar-xe"},"source":["> **注意：** 一开始不同的数据类型可能会让人感到困惑。但可以这样理解，数字越小（例如 32、16、8），计算机存储数值的精度就越低。由于存储量较小，通常会导致更快的计算和更小的整体模型。基于移动设备的神经网络通常使用 8 位整数，运行速度更快，但比 `float32` 更不准确。关于这方面的更多信息，可以阅读 [计算中的精度](https://en.wikipedia.org/wiki/Precision_%28computer_science%29)。\n","\n","> **练习：** 到目前为止，我们已经涵盖了很多张量方法，但在 [`torch.Tensor` 文档](https://pytorch.org/docs/stable/tensors.html) 中还有很多方法。我建议花 10 分钟浏览并查看任何引起你兴趣的内容。点击它们，然后自己编写代码看看会发生什么。\n"]},{"cell_type":"markdown","metadata":{"id":"7CkCtAYmGsHY"},"source":["### 重塑、堆叠、压缩和扩展\n","\n","在许多情况下，你可能需要重塑或更改张量的维度，而不实际更改其内部的值。\n","\n","为此，一些常用的方法如下：\n","\n","| 方法                                                                                                          | 简短描述                                                        |\n","| ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- |\n","| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | 将 `input` 重塑为 `shape`（如果兼容），也可以使用 `torch.Tensor.reshape()`。 |\n","| [`Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)                    | 返回一个不同 `shape` 的原始张量视图，但共享与原始张量相同的数据。                       |\n","| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html)                  | 在新的维度（`dim`）上连接一组 `tensors`，所有的 `tensors` 必须具有相同的大小。        |\n","| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html)                      | 压缩 `input`，去除所有维度值为 `1` 的维度。                                |\n","| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html)              | 返回在 `dim` 位置添加了维度值为 `1` 的 `input`。                          |\n","| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html)                | 返回一个视图，其中原始 `input` 的维度被重新排列为 `dims`。                       |\n","\n","为什么要使用这些方法？\n","\n","因为深度学习模型（神经网络）主要是通过某种方式操作张量。而且由于矩阵乘法的规则，如果张量形状不匹配，你会遇到错误。这些方法帮助你确保张量中的正确元素与其他张量中的正确元素进行混合。\n","\n","让我们来试一试。\n","\n","首先，我们将创建一个张量。\n","|"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYjRTLOzG4Ev","outputId":"c5ce820a-354f-4394-baea-f416d01b227a","executionInfo":{"status":"ok","timestamp":1757465222036,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"]},"metadata":{},"execution_count":53}],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1., 8.)\n","x, x.shape"]},{"cell_type":"markdown","metadata":{"id":"3_VarMO9CoT8"},"source":["现在让我们使用 `torch.reshape()` 来添加一个额外的维度。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"US4WjpQ3SG-8","outputId":"81b26f03-5fe6-415a-e82e-1669c05caf1c","executionInfo":{"status":"ok","timestamp":1757465247085,"user_tz":-480,"elapsed":89,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"]},"metadata":{},"execution_count":55}],"source":["# Add an extra dimension\n","x_reshaped = x.reshape(1, 7)\n","x_reshaped, x_reshaped.shape"]},{"cell_type":"markdown","metadata":{"id":"tig5xm0jCxuU"},"source":["我们也可以使用 `torch.view()` 来更改视图。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDN2BNe5TGfB","outputId":"e5650499-cec3-450e-f6d5-02e899afc468","executionInfo":{"status":"ok","timestamp":1757465251771,"user_tz":-480,"elapsed":29,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"]},"metadata":{},"execution_count":56}],"source":["# Change view (keeps same data as original but changes view)\n","# See more: https://stackoverflow.com/a/54507446/7900723\n","z = x.view(1, 7)\n","z, z.shape"]},{"cell_type":"markdown","metadata":{"id":"m8joAaUEC2NX"},"source":["不过请记住，通过 `torch.view()` 更改张量的视图实际上只是创建了 *同一个* 张量的新视图。\n","\n","因此，更改视图也会改变原始张量。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DxURVvXTGfC","outputId":"d50d173d-bb4d-4d2d-ffb2-f1947dd51298","executionInfo":{"status":"ok","timestamp":1757465284947,"user_tz":-480,"elapsed":30,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"]},"metadata":{},"execution_count":58}],"source":["# Changing z changes x\n","z[:, 0] = 5\n","z, x"]},{"cell_type":"markdown","metadata":{"id":"YxnqDBlpDDJ_"},"source":["如果我们想将新的张量堆叠在自己上面五次，可以使用 `torch.stack()` 来实现。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pX5Adf3ORiTK","outputId":"79ad344e-3953-4e85-b71d-576a82ffb7e3","executionInfo":{"status":"ok","timestamp":1757465293178,"user_tz":-480,"elapsed":18,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[5., 2., 3., 4., 5., 6., 7.],\n","        [5., 2., 3., 4., 5., 6., 7.],\n","        [5., 2., 3., 4., 5., 6., 7.],\n","        [5., 2., 3., 4., 5., 6., 7.]])"]},"metadata":{},"execution_count":59}],"source":["# Stack tensors on top of each other\n","x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n","x_stacked"]},{"cell_type":"code","source":["x_stacked = torch.stack([x, x, x, x], dim=1) # try changing dim to dim=1 and see what happens\n","x_stacked"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQZllqCnGEFd","executionInfo":{"status":"ok","timestamp":1757465307471,"user_tz":-480,"elapsed":38,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}},"outputId":"88a35220-08a8-4651-a5a1-febccb857916"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[5., 5., 5., 5.],\n","        [2., 2., 2., 2.],\n","        [3., 3., 3., 3.],\n","        [4., 4., 4., 4.],\n","        [5., 5., 5., 5.],\n","        [6., 6., 6., 6.],\n","        [7., 7., 7., 7.]])"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"ET56QzNHDuOI"},"source":["如果我们想从张量中移除所有单一维度，可以使用 `torch.squeeze()`。\n","\n","（我记得这是 *压缩* 张量，只保留大于 1 的维度）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2Y2HEoDRxJZ","outputId":"db294848-640e-4403-a11e-7f4c36af13c3","executionInfo":{"status":"ok","timestamp":1757465372774,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n","Previous shape: torch.Size([1, 7])\n","\n","New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n","New shape: torch.Size([7])\n"]}],"source":["print(f\"Previous tensor: {x_reshaped}\")\n","print(f\"Previous shape: {x_reshaped.shape}\")\n","\n","# Remove extra dimension from x_reshaped\n","x_squeezed = x_reshaped.squeeze()\n","print(f\"\\nNew tensor: {x_squeezed}\")\n","print(f\"New shape: {x_squeezed.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"acjDLk8WD8NC"},"source":["要执行 `torch.squeeze()` 的反操作，可以使用 `torch.unsqueeze()` 在特定索引位置添加一个维度值为 1 的维度。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUC-DEEwSYv7","outputId":"07fef613-ba9b-4f73-eebf-ac7c03a5e93b","executionInfo":{"status":"ok","timestamp":1757465375748,"user_tz":-480,"elapsed":67,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n","Previous shape: torch.Size([7])\n","\n","New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n","New shape: torch.Size([1, 7])\n"]}],"source":["print(f\"Previous tensor: {x_squeezed}\")\n","print(f\"Previous shape: {x_squeezed.shape}\")\n","\n","## Add an extra dimension with unsqueeze\n","x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n","print(f\"\\nNew tensor: {x_unsqueezed}\")\n","print(f\"New shape: {x_unsqueezed.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"R9DuJzXgFbM5"},"source":["你还可以使用 `torch.permute(input, dims)` 来重新排列轴的顺序，其中 `input` 被转换为一个具有新 `dims` 的 *视图*。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCRGCX8DTGfC","outputId":"0678aa5c-21f0-4d18-dda4-d31485a86b72","executionInfo":{"status":"ok","timestamp":1757465429699,"user_tz":-480,"elapsed":26,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous shape: torch.Size([224, 224, 3])\n","New shape: torch.Size([3, 224, 224])\n"]}],"source":["# Create tensor with specific shape\n","x_original = torch.rand(size=(224, 224, 3))\n","\n","# Permute the original tensor to rearrange the axis order\n","x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n","\n","print(f\"Previous shape: {x_original.shape}\")\n","print(f\"New shape: {x_permuted.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"06LKaFemGBoE"},"source":["> **注意**：因为 `permute` 返回的是一个 *视图*（与原始张量共享相同的数据），所以在转置后的张量中的值将与原始张量相同。如果你更改视图中的值，它也会改变原始张量中的值。\n"]},{"cell_type":"markdown","metadata":{"id":"nEPqVL7fTGfC"},"source":["## 索引（从张量中选择数据）\n","\n","有时你可能需要从张量中选择特定的数据（例如，仅选择第一列或第二行）。\n","\n","为此，你可以使用索引。\n","\n","如果你曾经在 Python 列表或 NumPy 数组中进行过索引，那么在 PyTorch 中使用张量索引非常类似。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSXzdxCQTGfD","outputId":"14855475-df23-4a55-942b-8fa55b435786","executionInfo":{"status":"ok","timestamp":1757465467055,"user_tz":-480,"elapsed":51,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[1, 2, 3],\n","          [4, 5, 6],\n","          [7, 8, 9]]]),\n"," torch.Size([1, 3, 3]))"]},"metadata":{},"execution_count":66}],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1, 10).reshape(1, 3, 3)\n","x, x.shape"]},{"cell_type":"markdown","metadata":{"id":"xQG5krnKG43B"},"source":["索引值的顺序是从外部维度到内部维度（查看方括号）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zv_Z3IAzTGfD","outputId":"522bfa06-d645-46e6-a4c0-eb0962311a56","executionInfo":{"status":"ok","timestamp":1757465497936,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["First square bracket:\n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","Second square bracket: tensor([1, 2, 3])\n","Third square bracket: 1\n"]}],"source":["# Let's index bracket by bracket\n","print(f\"First square bracket:\\n{x[0]}\")\n","print(f\"Second square bracket: {x[0][0]}\")\n","print(f\"Third square bracket: {x[0][0][0]}\")"]},{"cell_type":"markdown","metadata":{"id":"XaLjaIFxHe89"},"source":["你还可以使用 `:` 来指定“该维度中的所有值”，然后使用逗号（`,`）来添加另一个维度。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCT09pqeTGfD","outputId":"68aef9b1-3c0c-4e01-f7a8-2335071c506f","executionInfo":{"status":"ok","timestamp":1757465538733,"user_tz":-480,"elapsed":65,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3]])"]},"metadata":{},"execution_count":70}],"source":["# Get all values of 0th dimension and the 0 index of 1st dimension\n","x[:, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwDx_gMsTGfD","outputId":"7a715c80-2500-4027-d565-bb65884dce2e","executionInfo":{"status":"ok","timestamp":1757465539409,"user_tz":-480,"elapsed":31,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2, 5, 8]])"]},"metadata":{},"execution_count":71}],"source":["# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n","x[:, :, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiw3_1E3TGfD","outputId":"52764f2c-4dd9-4cfc-d435-29d981200a10","executionInfo":{"status":"ok","timestamp":1757465546962,"user_tz":-480,"elapsed":45,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5])"]},"metadata":{},"execution_count":72}],"source":["# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n","x[:, 1, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFVEgrKhTGfD","outputId":"ebbb1357-423e-4ab4-e89d-1db40376f894","executionInfo":{"status":"ok","timestamp":1757465548437,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":73}],"source":["# Get index 0 of 0th and 1st dimension and all values of 2nd dimension\n","x[0, 0, :] # same as x[0][0]"]},{"cell_type":"markdown","metadata":{"id":"6Ik0r11RIxtm"},"source":["索引一开始可能会有些困惑，尤其是在处理较大的张量时（我自己也经常需要多次尝试索引才能正确）。但是通过一些练习，并遵循数据探索者的座右铭（***可视化，可视化，再可视化***），你会开始掌握它的。\n"]},{"cell_type":"markdown","metadata":{"id":"h8ZaW0Bq7rCm"},"source":["## PyTorch 张量与 NumPy\n","\n","由于 NumPy 是一个流行的 Python 数值计算库，PyTorch 提供了与其良好交互的功能。\n","\n","你需要用到的两个主要方法用于 NumPy 和 PyTorch 之间的转换（以及反向转换）是：\n","\n","* [`torch.from_numpy(ndarray)`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - NumPy 数组 -> PyTorch 张量。\n","* [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) - PyTorch 张量 -> NumPy 数组。\n","\n","让我们来试一试这些方法。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDrDCnvY7rKS","outputId":"d85563f6-2527-4e8b-e03f-c1dc0f39c8f4","executionInfo":{"status":"ok","timestamp":1757465565194,"user_tz":-480,"elapsed":35,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1., 2., 3., 4., 5., 6., 7.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"metadata":{},"execution_count":74}],"source":["# NumPy array to tensor\n","import torch\n","import numpy as np\n","array = np.arange(1.0, 8.0)\n","tensor = torch.from_numpy(array)\n","array, tensor"]},{"cell_type":"markdown","metadata":{"id":"16JG6cONLPnO"},"source":["> **注意：** 默认情况下，NumPy 数组的创建数据类型是 `float64`，如果你将其转换为 PyTorch 张量，它将保持相同的数据类型（如上所示）。\n",">\n","> 然而，许多 PyTorch 计算默认使用 `float32`。\n",">\n","> 所以，如果你想将 NumPy 数组（`float64`） -> PyTorch 张量（`float64`） -> PyTorch 张量（`float32`），你可以使用 `tensor = torch.from_numpy(array).type(torch.float32)`。\n","\n","因为我们之前重新赋值了 `tensor`，如果你更改张量，数组会保持不变。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovwl7VCREv8L","outputId":"5dda61f8-9713-43ae-e7b5-13333f144b9b","executionInfo":{"status":"ok","timestamp":1757465625007,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([3., 4., 5., 6., 7., 8., 9.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"metadata":{},"execution_count":76}],"source":["# Change the array, keep the tensor\n","array = array + 1\n","array, tensor"]},{"cell_type":"markdown","metadata":{"id":"geVvu1p0MTWc"},"source":["如果你想从 PyTorch 张量转换为 NumPy 数组，可以调用 `tensor.numpy()`。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xw_7ZyVaTKxQ","outputId":"3100203c-4309-4f00-b8b3-25b17967de14","executionInfo":{"status":"ok","timestamp":1757465648650,"user_tz":-480,"elapsed":30,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 1., 1., 1., 1., 1., 1.]),\n"," array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"]},"metadata":{},"execution_count":78}],"source":["# Tensor to NumPy array\n","tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n","numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n","tensor, numpy_tensor"]},{"cell_type":"markdown","metadata":{"id":"Dt8yEV1jMfi2"},"source":["同样的规则也适用，如果你更改了原始的 `tensor`，新的 `numpy_tensor` 将保持不变。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMp6ZSkET4_Y","outputId":"a555fdc1-d332-4780-819d-cc8e368136f1","executionInfo":{"status":"ok","timestamp":1757465649731,"user_tz":-480,"elapsed":41,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([2., 2., 2., 2., 2., 2., 2.]),\n"," array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"]},"metadata":{},"execution_count":79}],"source":["# Change the tensor, keep the array the same\n","tensor = tensor + 1\n","tensor, numpy_tensor"]},{"cell_type":"markdown","metadata":{"id":"7gU3ubCrUkI-"},"source":["## 可重复性（尽量去除随机性）\n","\n","随着你深入学习神经网络和机器学习，你会开始发现随机性在其中扮演了多么重要的角色。\n","\n","嗯，实际上是伪随机性。毕竟，按照设计，计算机本质上是确定性的（每一步都是可以预测的），因此它们产生的随机性是模拟的随机性（虽然对此也有争议，但既然我不是计算机科学家，我会让你自己去了解更多）。\n","\n","那么这和神经网络和深度学习有什么关系呢？\n","\n","我们已经讨论过，神经网络以随机数开始，用来描述数据中的模式（这些数字是对数据的较差描述），然后通过张量操作（以及一些我们尚未讨论的其他方法）来改善这些随机数字，从而更好地描述数据中的模式。\n","\n","简而言之：\n","\n","```\n","从随机数字开始 -> 张量操作 -> 尝试不断改善（一次又一次）\n","```\n","\n","尽管随机性是强大且有用的，但有时你希望减少一些随机性。\n","\n","为什么？\n","\n","这样你就可以进行可重复的实验。\n","\n","例如，你创建了一个算法，能够达到 X 性能。\n","\n","然后，你的朋友试试看，验证你是不是疯了。\n","\n","他们怎么能做到这一点呢？\n","\n","这就是 **可重复性** 的作用。\n","\n","换句话说，你能在自己的计算机上运行相同的代码并得到和我相似的结果吗？\n","\n","让我们来看一个简短的 PyTorch 中可重复性的示例。\n","\n","我们将首先创建两个随机张量，既然它们是随机的，你会期望它们是不同的，对吧？\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSwxnwEbTGfF","outputId":"c1e5def4-72c1-474f-ed5b-8590c34d3e17","executionInfo":{"status":"ok","timestamp":1757465682842,"user_tz":-480,"elapsed":31,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor A:\n","tensor([[0.8660, 0.2001, 0.7926, 0.1140],\n","        [0.6855, 0.0086, 0.3588, 0.9211],\n","        [0.8627, 0.8223, 0.5248, 0.5881]])\n","\n","Tensor B:\n","tensor([[0.2746, 0.3791, 0.2975, 0.6450],\n","        [0.0163, 0.9160, 0.5888, 0.8403],\n","        [0.4761, 0.7140, 0.4644, 0.6362]])\n","\n","Does Tensor A equal Tensor B? (anywhere)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[False, False, False, False],\n","        [False, False, False, False],\n","        [False, False, False, False]])"]},"metadata":{},"execution_count":80}],"source":["import torch\n","\n","# Create two random tensors\n","random_tensor_A = torch.rand(3, 4)\n","random_tensor_B = torch.rand(3, 4)\n","\n","print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n","print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n","print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n","random_tensor_A == random_tensor_B"]},{"cell_type":"markdown","metadata":{"id":"nPU6mDKJnr8M"},"source":["正如你可能预期的那样，这两个张量的值是不同的。\n","\n","但是，如果你想创建两个具有 *相同* 值的随机张量呢？\n","\n","也就是说，这些张量仍然包含随机值，但它们的随机性是相同的。\n","\n","这时，[`torch.manual_seed(seed)`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) 就派上用场了，其中 `seed` 是一个整数（像 `42` 这样的数字，但可以是任何数字），它决定了随机性的种子。\n","\n","让我们通过创建一些带有 *相同种子* 的随机张量来试试吧。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sB6d1GfYTGfF","outputId":"3400ab25-33d0-4121-9f71-c54721580307","executionInfo":{"status":"ok","timestamp":1757465713838,"user_tz":-480,"elapsed":74,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor C:\n","tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n","        [0.3904, 0.6009, 0.2566, 0.7936],\n","        [0.9408, 0.1332, 0.9346, 0.5936]])\n","\n","Tensor D:\n","tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n","        [0.3904, 0.6009, 0.2566, 0.7936],\n","        [0.9408, 0.1332, 0.9346, 0.5936]])\n","\n","Does Tensor C equal Tensor D? (anywhere)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True]])"]},"metadata":{},"execution_count":81}],"source":["import torch\n","import random\n","\n","# # Set the random seed\n","RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n","torch.manual_seed(seed=RANDOM_SEED)\n","random_tensor_C = torch.rand(3, 4)\n","\n","# Have to reset the seed every time a new rand() is called\n","# Without this, tensor_D would be different to tensor_C\n","torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n","random_tensor_D = torch.rand(3, 4)\n","\n","print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n","print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n","print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n","random_tensor_C == random_tensor_D"]},{"cell_type":"markdown","metadata":{"id":"uct53Xr5QRC_"},"source":["太棒了！\n","\n","看起来设置种子确实有效。\n","\n","> **资源：** 我们刚刚讨论的只是 PyTorch 中可重复性的冰山一角。关于可重复性和随机种子，更多的信息可以参考：\n",">\n","> * [PyTorch 可重复性文档](https://pytorch.org/docs/stable/notes/randomness.html)（一个不错的练习是花 10 分钟阅读它，即使现在不完全理解，了解它也很重要）。\n","> * [维基百科的随机种子页面](https://en.wikipedia.org/wiki/Random_seed)（这将为你提供关于随机种子和伪随机性的总体概述）。\n"]},{"cell_type":"markdown","metadata":{"id":"hxIIM7t27rQ-"},"source":["## 在 GPU 上运行张量（并加速计算）\n","\n","深度学习算法需要大量的数值运算。\n","\n","默认情况下，这些操作通常是在 CPU（中央处理单元）上执行的。\n","\n","然而，还有另一种常见的硬件——GPU（图形处理单元），它在执行神经网络所需的特定类型操作（如矩阵乘法）时，通常比 CPU 快得多。\n","\n","你的计算机可能配有 GPU。\n","\n","如果有的话，你应该尽可能利用它来训练神经网络，因为它可能会显著加快训练时间。\n","\n","有几种方法可以首先获得对 GPU 的访问权限，然后让 PyTorch 使用 GPU。\n","\n","> **注意：** 在本课程中提到 \"GPU\" 时，指的是启用了 [Nvidia GPU 和 CUDA](https://developer.nvidia.com/cuda-gpus) 的 GPU（CUDA 是一个计算平台和 API，帮助 GPU 用于通用计算，而不仅仅是图形处理），除非另有说明。\n"]},{"cell_type":"markdown","metadata":{"id":"0UiR6QpoYQH_"},"source":["### 1. 获取 GPU\n","\n","当我提到 GPU 时，你可能已经知道是什么意思。如果不知道，下面有几种方法可以获得 GPU 访问权限。\n","\n","| **方法**               | **设置难度** | **优点**                        | **缺点**                  | **如何设置**                                                                    |\n","| -------------------- | -------- | ----------------------------- | ----------------------- | --------------------------------------------------------------------------- |\n","| Google Colab         | 简单       | 免费使用，几乎不需要设置，分享工作就像分享一个链接一样简单 | 不保存数据输出，计算能力有限，可能会超时    | [遵循 Google Colab 指南](https://colab.research.google.com/notebooks/gpu.ipynb) |\n","| 使用自己的设备              | 中等       | 所有操作都在本地计算机上进行                | GPU 需要付费，前期有成本          | 按照 [PyTorch 安装指南](https://pytorch.org/get-started/locally/)                 |\n","| 云计算（AWS, GCP, Azure） | 中等-困难    | 小的前期成本，几乎无限的计算能力              | 如果持续运行，成本可能会很高，需要一些时间设置 | 按照 [PyTorch 云计算安装指南](https://pytorch.org/get-started/cloud-partners/)       |\n","\n","还有更多使用 GPU 的选项，但上述三种方式现在已经足够。\n","\n","就个人而言，我使用 Google Colab 和我自己的个人计算机进行小规模实验（以及创建这门课程），当需要更多计算能力时，我会使用云计算资源。\n","\n","> **资源：** 如果你想购买自己的 GPU，但不确定该选择哪个， [Tim Dettmers 有一篇很棒的指南](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)。\n","\n","要检查你是否有 Nvidia GPU 的访问权限，你可以运行 `!nvidia-smi`，其中 `!`（也叫做 bang）表示“在命令行上运行此命令”。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEMcO-9zYc-w","outputId":"bdb1a1e6-b130-4508-ab50-831771b77715","executionInfo":{"status":"ok","timestamp":1757465748958,"user_tz":-480,"elapsed":104,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Sep 10 00:55:49 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"HvkB9p5zYf8E"},"source":["如果你无法访问 Nvidia GPU，上面的输出可能类似于：\n","\n","```\n","NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","```\n","\n","在这种情况下，请回到上面的步骤并按照安装指南进行操作。\n","\n","如果你确实有 GPU，以上输出会类似于：\n","\n","```\n","Wed Jan 19 22:09:08 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"UvibZ6e0YcDk"},"source":["### 2. 让 PyTorch 在 GPU 上运行\n","\n","一旦你准备好访问 GPU，下一步就是让 PyTorch 使用 GPU 来存储数据（张量）并在数据上进行计算（对张量执行操作）。\n","\n","为此，你可以使用 [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) 包。\n","\n","与其仅仅讲解，让我们直接试一试。\n","\n","你可以使用 [`torch.cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available) 来测试 PyTorch 是否可以访问 GPU。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OweDLgwjEvZ2","outputId":"a64ae78e-2825-415b-da81-c2099e1ca8ec","executionInfo":{"status":"ok","timestamp":1757465769901,"user_tz":-480,"elapsed":23,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":83}],"source":["# Check for GPU\n","import torch\n","torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"jedZcx2PZFpL"},"source":["如果上述输出是 `True`，说明 PyTorch 可以看到并使用 GPU；如果输出是 `False`，说明它无法看到 GPU，在这种情况下，你需要回到安装步骤进行检查。\n","\n","现在，假设你想设置代码，使其在 CPU *或* GPU（如果可用）上运行。\n","\n","这样，无论你自己还是别人决定运行你的代码，它都能在他们使用的计算设备上正常工作。\n","\n","让我们创建一个 `device` 变量来存储可用的设备类型。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"j92HBCKB7rYa","outputId":"edda6238-0426-4bd0-c4b5-ccb2ba30bd0d","executionInfo":{"status":"ok","timestamp":1757465778336,"user_tz":-480,"elapsed":35,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":84}],"source":["# Set device type\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"FjFyPP2WaCch"},"source":["如果上述输出是 `\"cuda\"`，这意味着我们可以将所有 PyTorch 代码设置为使用可用的 CUDA 设备（即 GPU），如果输出是 `\"cpu\"`，则 PyTorch 代码将继续使用 CPU。\n","\n","> **注意：** 在 PyTorch 中，最佳实践是编写 [**设备无关的代码**](https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code)。这意味着代码能够在 CPU（始终可用）或 GPU（如果可用）上运行。\n","\n","如果你想进行更快的计算，你可以使用 GPU，但如果你想进行 *更* 快的计算，你可以使用多个 GPU。\n","\n","你可以使用 [`torch.cuda.device_count()`](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count) 来计算 PyTorch 可以访问的 GPU 数量。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MArsn0DFTGfG","outputId":"54d4b6ec-5bee-4a6c-de89-bfa3cd562ade","executionInfo":{"status":"ok","timestamp":1757465796937,"user_tz":-480,"elapsed":15,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":85}],"source":["# Count number of devices\n","torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"xVNf1hiqa-gO"},"source":["知道 PyTorch 可以访问的 GPU 数量是很有帮助的，特别是当你想在一个 GPU 上运行一个特定的过程，在另一个 GPU 上运行另一个过程时（PyTorch 也提供了让你在 *所有* GPU 上运行一个过程的功能）。\n"]},{"cell_type":"markdown","metadata":{"id":"gnvvzKG1Qehb"},"source":["### 2.1 在 Apple Silicon 上运行 PyTorch\n","\n","为了在 Apple 的 M1/M2/M3 GPU 上运行 PyTorch，你可以使用 [`torch.backends.mps`](https://pytorch.org/docs/stable/notes/mps.html) 模块。\n","\n","确保你的 macOS 和 PyTorch 版本是最新的。\n","\n","你可以使用 `torch.backends.mps.is_available()` 来测试 PyTorch 是否能够访问 GPU。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPpOP_EQQehb","outputId":"8e0f7020-6b42-4e8b-8327-11283b3c7e34","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757465808303,"user_tz":-480,"elapsed":15,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":86}],"source":["# Check for Apple Silicon GPU\n","import torch\n","torch.backends.mps.is_available() # Note this will print false if you're not running on a Mac"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5MCD33vQehb","outputId":"8d3598bc-0f89-47b7-8a69-132388af6982","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1757465809807,"user_tz":-480,"elapsed":18,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":87}],"source":["# Set device type\n","device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"LzcvyrdGQehb"},"source":["如前所述，如果上述输出是 `\"mps\"`，这意味着我们可以将所有 PyTorch 代码设置为使用可用的 Apple Silicon GPU。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsijEImAQehb"},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = \"cuda\" # Use NVIDIA GPU (if available)\n","elif torch.backends.mps.is_available():\n","    device = \"mps\" # Use Apple Silicon GPU (if available)\n","else:\n","    device = \"cpu\" # Default to CPU if no GPU is available"]},{"cell_type":"markdown","metadata":{"id":"XqQLcuj68OA-"},"source":["### 3. 将张量（和模型）放到 GPU 上\n","\n","你可以通过在张量上调用 [`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) 方法，将张量（以及模型，我们稍后会看到）放到特定的设备上，其中 `device` 是你希望张量（或模型）放置的目标设备。\n","\n","为什么要这样做？\n","\n","GPU 提供的数值计算速度比 CPU 快得多，如果 GPU 不可用，基于我们的 **设备无关代码**（见上文），它将会在 CPU 上运行。\n","\n","> **注意：** 使用 `to(device)` 将张量放到 GPU 上（例如 `some_tensor.to(device)`）会返回该张量的副本，即同一个张量会存在于 CPU 和 GPU 上。要覆盖张量，重新赋值它们：\n",">\n","> `some_tensor = some_tensor.to(device)`\n","\n","让我们尝试创建一个张量，并将它放到 GPU 上（如果可用）。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhI3srFXEHfP","outputId":"29f20329-82e4-4d83-e8d4-669d1ae78c14","executionInfo":{"status":"ok","timestamp":1757465863520,"user_tz":-480,"elapsed":37,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) cpu\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":91}],"source":["# Create tensor (default on CPU)\n","tensor = torch.tensor([1, 2, 3])\n","\n","# Tensor not on GPU\n","print(tensor, tensor.device)\n","\n","# Move tensor to GPU (if available)\n","tensor_on_gpu = tensor.to(device)\n","tensor_on_gpu"]},{"cell_type":"markdown","metadata":{"id":"DxXeRKO0TGfG"},"source":["如果你有可用的 GPU，以上代码将输出类似如下：\n","\n","```\n","tensor([1, 2, 3]) cpu\n","tensor([1, 2, 3], device='cuda:0')\n","```\n","\n","注意第二个张量有 `device='cuda:0'`，这意味着它存储在可用的第 0 个 GPU 上（GPU 是从 0 开始索引的，如果有两个 GPU 可用，它们将分别是 `'cuda:0'` 和 `'cuda:1'`，依此类推，直到 `'cuda:n'`）。\n"]},{"cell_type":"markdown","metadata":{"id":"4puyUX4Bci5D"},"source":["### 4. 将张量移动回 CPU\n","\n","如果我们想将张量移回 CPU，该怎么做呢？\n","\n","例如，如果你想用 NumPy 与张量进行交互时，就需要这样做（因为 NumPy 不使用 GPU）。\n","\n","让我们尝试在我们的 `tensor_on_gpu` 上使用 [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) 方法。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"3ChSLJgPTGfG","outputId":"6e0bba6a-012f-425d-e735-6853014972bc","executionInfo":{"status":"error","timestamp":1757465871208,"user_tz":-480,"elapsed":25,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-704146591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}],"source":["# If tensor is on GPU, can't transform it to NumPy (this will error)\n","tensor_on_gpu.numpy()"]},{"cell_type":"markdown","metadata":{"id":"LhymtkRDTGfG"},"source":["相反，要将张量移回 CPU 并与 NumPy 一起使用，我们可以使用 [`Tensor.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html)。\n","\n","这会将张量复制到 CPU 内存中，使其可以在 CPU 上使用。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gN15s-NdTGfG","outputId":"b8306c08-ff1d-4eed-cbb3-116cdead47ce","executionInfo":{"status":"ok","timestamp":1757465924209,"user_tz":-480,"elapsed":52,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":93}],"source":["# Instead, copy the tensor back to cpu\n","tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n","tensor_back_on_cpu"]},{"cell_type":"markdown","metadata":{"id":"qyzNH5lrTGfH"},"source":["上述操作会返回一个在 CPU 内存中的 GPU 张量副本，因此原始张量仍然保留在 GPU 上。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5u83PCRTGfH","outputId":"11eca024-564e-4bab-a0be-6017f29e0597","executionInfo":{"status":"ok","timestamp":1757465928808,"user_tz":-480,"elapsed":47,"user":{"displayName":"Yi Yang","userId":"15270580007541007571"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":94}],"source":["tensor_on_gpu"]},{"cell_type":"markdown","metadata":{"id":"xlmBpnuPTGfH"},"source":["## 练习\n","\n","所有的练习都专注于实践上面提到的代码。\n","\n","你应该能够通过参考每一部分或通过跟随链接的资源来完成这些练习。\n","\n","**资源：**\n","\n","* [00 的练习模板笔记本](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/00_pytorch_fundamentals_exercises.ipynb)。\n","* [00 的示例解答笔记本](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/00_pytorch_fundamentals_exercise_solutions.ipynb)（在查看这个之前先尝试做这些练习）。\n","\n","1. **文档阅读** - 深度学习（以及学习编程的一大部分）是要熟悉你所使用的框架的文档。我们将在本课程的剩余部分中大量使用 PyTorch 文档。因此，我建议花 10 分钟阅读以下内容（如果你暂时不完全理解一些内容也没关系，重点是意识到它们）。查看 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) 和 [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics) 的文档。\n","2. 创建一个形状为 `(7, 7)` 的随机张量。\n","3. 对第 2 步的张量与另一个形状为 `(1, 7)` 的随机张量执行矩阵乘法（提示：你可能需要对第二个张量进行转置）。\n","4. 将随机种子设置为 `0`，然后重新执行第 2 步和第 3 步的练习。\n","5. 说到随机种子，我们看到了如何通过 `torch.manual_seed()` 来设置它，但是否有 GPU 的等效方法？（提示：你需要查看 `torch.cuda` 的文档）。如果有的话，将 GPU 随机种子设置为 `1234`。\n","6. 创建两个形状为 `(2, 3)` 的随机张量并将它们都发送到 GPU（你需要有 GPU 访问权限）。在创建张量时设置 `torch.manual_seed(1234)`（这不需要是 GPU 随机种子）。\n","7. 对第 6 步创建的张量执行矩阵乘法（同样，你可能需要调整其中一个张量的形状）。\n","8. 找出第 7 步输出的最大值和最小值。\n","9. 找出第 7 步输出的最大值和最小值的索引。\n","10. 创建一个形状为 `(1, 1, 1, 10)` 的随机张量，然后创建一个去除所有 `1` 维度的新张量，最终得到一个形状为 `(10)` 的张量。在创建时设置种子为 `7`，并打印出第一个张量及其形状，以及第二个张量及其形状。\n"]},{"cell_type":"markdown","metadata":{"id":"_DHY3NCcQehd"},"source":["## 课外活动\n","\n","* 花 1 小时浏览 [PyTorch 基础教程](https://pytorch.org/tutorials/beginner/basics/intro.html)（我推荐先看 [快速入门](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) 和 [张量](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) 部分）。\n","* 要了解更多关于张量如何表示数据的信息，请观看这个视频：[什么是张量？](https://youtu.be/f5liqUk0ZTw)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"interpreter":{"hash":"3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}