# 帮我写出onnx 和 tensorRT的加速方法

下面分别给出**ONNX**和**TensorRT**的推理加速方法示例，适用于你的 PyTorch 模型。

---

### 1. PyTorch 导出 ONNX 并用 ONNX Runtime 推理

```python
import torch
import torch.nn as nn

# 假设你的模型和输入
model = M()
model.eval()
dummy_input = torch.randn(1, 4)  # 注意：输入形状要和模型 forward 匹配

# 导出为 ONNX
torch.onnx.export(model, dummy_input, "model.onnx", input_names=["input"], output_names=["output"], opset_version=12)

# 用 ONNX Runtime 推理
import onnxruntime as ort
import numpy as np

ort_session = ort.InferenceSession("model.onnx")
input_data = np.random.randn(1, 4).astype(np.float32)
outputs = ort_session.run(None, {"input": input_data})
print(outputs[0])
```

---

### 2. 用 TensorRT 加速 ONNX 模型（命令行和 Python API）

#### 方法一：命令行（推荐初学者）
```shell
trtexec --onnx=model.onnx --saveEngine=model.trt --fp16
```
- 这会生成 TensorRT 引擎文件 `model.trt`，支持 FP16 加速。

#### 方法二：Python API（需安装 tensorrt）
```python
import tensorrt as trt

TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
with open("model.onnx", "rb") as f, trt.Builder(TRT_LOGGER) as builder, \
     builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
     trt.OnnxParser(network, TRT_LOGGER) as parser:
    parser.parse(f.read())
    builder.max_batch_size = 1
    builder.max_workspace_size = 1 << 30
    engine = builder.build_cuda_engine(network)
    # 后续可用 pycuda 进行推理
```
> TensorRT Python 推理较复杂，推荐先用 ONNX Runtime 或 trtexec 命令行。

---

**注意事项：**
- 输入输出 shape 必须和模型一致。
- TensorRT 需要 NVIDIA GPU 和 CUDA 环境。
- ONNX Runtime 支持 CPU/GPU，部署简单。

# 帮我整理一下所有的推理方法

下面整理常见的**推理加速方法**，适用于CV模型（如YOLOv5、Swin Transformer）和NLP大模型（LLM，如GPT/BERT等）：

---

## 1. **原生 PyTorch/Transformers 推理**
- 直接用 `.eval()` 模式和 `with torch.no_grad()`。
- 适合开发和小规模部署。

---

## 2. **量化（Quantization）**
- **动态量化**（Dynamic Quantization）：适合NLP（如BERT、LLM），对Linear层等权重做int8/float16。
- **静态量化**（Static Quantization）：适合CV，需校准数据。
- **工具**：`torch.ao.quantization`、`transformers`的`quantization_config`、`bitsandbytes`（LLM 4/8bit量化）。

---

## 3. **ONNX 导出与 ONNX Runtime**
- 将模型导出为ONNX格式，用ONNX Runtime推理。
- 支持CPU/GPU/多平台，速度快，兼容性好。
- LLM也支持（如`optimum.onnxruntime`）。

---

## 4. **TensorRT 加速**
- NVIDIA GPU专用，支持FP16/INT8，极致加速。
- 适合CV和NLP大模型（如Swin、YOLO、LLM）。
- 可用trtexec命令行或TensorRT Python API。

---

## 5. **混合精度（FP16/BF16）推理**
- 用AMP（自动混合精度）或模型半精度权重。
- 提升速度，显存占用低，精度损失小。
- LLM和CV模型均支持。

---

## 6. **模型剪枝/蒸馏**
- 剪枝：去除冗余参数，减小模型体积。
- 蒸馏：大模型指导小模型，提升小模型效果。
- 适合对速度和体积有极致要求场景。

---

## 7. **专用推理引擎/库**
- **HuggingFace Optimum**：一键加速LLM（支持ONNX、TensorRT、OpenVINO等）。
- **vLLM**、**TGI**、**FasterTransformer**：大模型推理专用，支持多卡、KV缓存、分布式。
- **OpenVINO**：Intel平台推理加速。
- **DeepSpeed-Inference**：LLM高效推理。

---

## 8. **批量推理/并发推理**
- 一次处理多条输入，提升吞吐量。
- 适合服务器端部署。

---

## 9. **LoRA/Adapter推理**
- 只加载LoRA/Adapter权重，主模型冻结，推理快且灵活。
- 适合多任务/多领域LLM。

---

## 10. **云服务/硬件加速**
- 利用云端推理API（如Azure、AWS、百度飞桨等）。
- TPU/NPU等专用硬件。

---

### **总结表**

| 方法         | 适用场景   | 工具/库举例                  |
| ------------ | ---------- | ---------------------------- |
| 原生PyTorch  | 通用       | torch, transformers          |
| 量化         | CV/NLP/LLM | torch.ao, bitsandbytes       |
| ONNX Runtime | 通用       | onnx, onnxruntime, optimum   |
| TensorRT     | NVIDIA GPU | tensorrt, trtexec            |
| 混合精度     | 通用       | torch.cuda.amp, transformers |
| 剪枝/蒸馏    | 通用       | torch-pruning, distilbert    |
| 专用推理引擎 | LLM/大模型 | vllm, TGI, DeepSpeed         |
| 批量/并发    | 服务器     | torch, transformers          |
| LoRA/Adapter | LLM多任务  | peft, transformers           |
| 云/硬件加速  | 通用       | Azure, AWS, OpenVINO         |

---



# 减枝和蒸馏

下面详细介绍**剪枝（Pruning）**和**蒸馏（Distillation）**的原理、常用流程，并给出代码示例。

---

## 一、剪枝（Pruning）

### 原理简介
剪枝是指将神经网络中不重要的参数（如权重较小的连接）置零或移除，从而减少模型体积和计算量，加速推理。常见剪枝方式有非结构化剪枝（单个权重）、结构化剪枝（整个通道/神经元）。

### PyTorch 剪枝代码示例

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune

# 定义一个简单的模型
class M(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(4, 4)
    def forward(self, x):
        return self.fc(x)

model = M()
print("原始权重：\n", model.fc.weight)

# 对fc层进行L1非结构化剪枝，剪掉30%的权重
prune.l1_unstructured(model.fc, name="weight", amount=0.3)
print("剪枝后权重（带掩码）：\n", model.fc.weight)
print("剪枝掩码：\n", model.fc.weight_mask)

# 剪枝后推理
x = torch.randn(2, 4)
with torch.no_grad():
    y = model(x)
print("剪枝后输出：\n", y)

# 移除掩码，真正稀疏化
prune.remove(model.fc, 'weight')
print("移除掩码后权重：\n", model.fc.weight)
```

**常见剪枝方式：**
- `prune.l1_unstructured`：按L1范数剪权重
- `prune.random_unstructured`：随机剪
- `prune.ln_structured`：按通道/神经元剪

---

## 二、蒸馏（Distillation）

### 原理简介
知识蒸馏是用大模型（Teacher）的输出（软标签）指导小模型（Student）训练，让小模型学习大模型的知识，提升小模型的性能。

### 基本流程
1. 用Teacher模型对输入数据推理，得到soft label（概率分布）。
2. Student模型同时学习真实标签和Teacher输出。
3. 损失函数通常为：蒸馏损失（KL散度）+ 真实标签损失（交叉熵）。

### 代码示例

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 假设有teacher和student模型
teacher = nn.Linear(4, 4)
student = nn.Linear(4, 4)

# 假设teacher已训练好，student待训练
teacher.eval()
student.train()

optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)
T = 2.0  # 蒸馏温度
alpha = 0.7  # 蒸馏损失权重

for step in range(10):
    x = torch.randn(8, 4)
    label = torch.randint(0, 4, (8,))

    with torch.no_grad():
        teacher_logits = teacher(x)
    student_logits = student(x)

    # 蒸馏损失（KL散度，soft label）
    loss_kd = F.kl_div(
        F.log_softmax(student_logits / T, dim=1),
        F.softmax(teacher_logits / T, dim=1),
        reduction='batchmean'
    ) * (T * T)

    # 真实标签损失
    loss_ce = F.cross_entropy(student_logits, label)

    # 总损失
    loss = alpha * loss_kd + (1 - alpha) * loss_ce

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Step {step}, Loss: {loss.item():.4f}")
```

---

## 总结

- **剪枝**：减少参数量和计算量，提升推理速度，适合部署。
- **蒸馏**：提升小模型精度，适合模型压缩和迁移。

