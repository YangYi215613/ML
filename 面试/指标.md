# 评价指标

下面总结深度学习中常见的**评价指标**，并说明每个指标适用的任务及其含义：

---

## 1. 分类任务

| 指标                   | 含义                                         | 适用任务                  |
| ---------------------- | -------------------------------------------- | ------------------------- |
| Accuracy（准确率）     | 正确预测样本数占总样本数的比例。             | 二/多分类                 |
| Precision（精确率）    | 预测为正的样本中，真正例的比例。             | 二/多分类，尤其关注误报时 |
| Recall（召回率）       | 所有正样本中被正确预测为正的比例。           | 二/多分类，尤其关注漏报时 |
| F1-score               | 精确率和召回率的调和平均数，兼顾误报和漏报。 | 二/多分类                 |
| ROC-AUC                | 反映模型区分正负样本能力，曲线下的面积。     | 二分类                    |
| Top-k Accuracy         | 预测结果的前k个中包含正确标签的比例。        | 多分类（如ImageNet）      |
| Log Loss/Cross Entropy | 预测概率与真实标签的差异，越小越好。         | 分类                      |

---

## 2. 回归任务

| 指标                | 含义                                                     | 适用任务 |
| ------------------- | -------------------------------------------------------- | -------- |
| MSE（均方误差）     | 预测值与真实值差的平方的平均值。                         | 回归     |
| RMSE（均方根误差）  | MSE的平方根，单位与原数据一致。                          | 回归     |
| MAE（平均绝对误差） | 预测值与真实值差的绝对值的平均值。                       | 回归     |
| R²（决定系数）      | 反映模型对数据方差的解释能力，1为完美拟合，0为无解释力。 | 回归     |

---

## 3. 目标检测

| 指标                          | 含义                                             | 适用任务 |
| ----------------------------- | ------------------------------------------------ | -------- |
| mAP（mean Average Precision） | 多类别平均精度，综合考虑检测框的准确性和召回率。 | 目标检测 |
| IoU（交并比）                 | 检测框与真实框的重叠程度，越高越好。             | 目标检测 |
| Precision/Recall/F1           | 同分类任务，但以检测框为单位。                   | 目标检测 |

---

## 4. 语义分割/实例分割

| 指标                                 | 含义                                              | 适用任务      |
| ------------------------------------ | ------------------------------------------------- | ------------- |
| mIoU（mean Intersection over Union） | 所有类别IoU的平均值。                             | 语义/实例分割 |
| Pixel Accuracy                       | 正确分类像素占总像素比例。                        | 语义分割      |
| Dice Coefficient                     | 2*交集/（预测+真实），与IoU类似，常用于医学分割。 | 语义/实例分割 |

---

## 5. 生成任务（如文本、图像）

| 指标                              | 含义                                           | 适用任务           |
| --------------------------------- | ---------------------------------------------- | ------------------ |
| BLEU                              | 机器翻译中，预测文本与参考文本的n-gram重叠度。 | 机器翻译、文本生成 |
| ROUGE                             | 预测文本与参考文本的召回率，常用于摘要。       | 文本摘要           |
| CIDEr                             | 图像描述任务，衡量生成描述与参考描述的相似度。 | 图像描述           |
| Inception Score                   | 生成图像的多样性和质量。                       | 图像生成           |
| FID（Fréchet Inception Distance） | 生成图像与真实图像分布的距离，越小越好。       | 图像生成           |

---

## 6. 检索/排序任务

| 指标                          | 含义                           | 适用任务   |
| ----------------------------- | ------------------------------ | ---------- |
| MAP（Mean Average Precision） | 检索结果的平均精度。           | 检索、排序 |
| NDCG（归一化折损累计增益）    | 排序相关性评价，考虑位置影响。 | 检索、排序 |
| Hit@k                         | 前k个结果中是否命中目标。      | 检索、推荐 |

---

## 7. 序列标注任务（如NER、分词）

| 指标                | 含义                                    | 适用任务           |
| ------------------- | --------------------------------------- | ------------------ |
| Precision/Recall/F1 | 以实体/标签为单位的精确率、召回率、F1。 | NER、分词、POS标注 |

---

# 激活函数

深度学习中常用的激活函数有：

1. **ReLU（Rectified Linear Unit）**
   - 公式：`f(x) = max(0, x)`
   - 特点：简单高效，广泛用于卷积和全连接层。

2. **Leaky ReLU**
   - 公式：`f(x) = x if x > 0 else αx`（α一般取0.01）
   - 特点：解决ReLU“死亡”问题。

3. **Sigmoid**
   - 公式：`f(x) = 1 / (1 + exp(-x))`
   - 特点：输出范围(0, 1)，常用于二分类输出层。

4. **Tanh（双曲正切）**
   - 公式：`f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
   - 特点：输出范围(-1, 1)，常用于RNN等。

5. **Softmax**
   - 公式：`f(x_i) = exp(x_i) / sum(exp(x_j))`
   - 特点：多分类输出层，将输出归一化为概率分布。

6. **Swish**
   - 公式：`f(x) = x * sigmoid(x)`
   - 特点：Google提出，部分模型表现优于ReLU。

7. **GELU（Gaussian Error Linear Unit）**
   - 公式：`f(x) = x * Φ(x)`（Φ为标准正态分布的累积分布函数）
   - 特点：BERT、Transformer等模型常用。

8. **ELU（Exponential Linear Unit）**
   - 公式：`f(x) = x if x > 0 else α*(exp(x)-1)`
   - 特点：缓解ReLU的负区间问题。

---

# 优化器

深度学习中常用的优化器及其适用任务如下：

---

### 1. **SGD（随机梯度下降）**
- **全称**：Stochastic Gradient Descent
- **特点**：基础优化器，可加Momentum（动量）提升收敛速度。
- **适用任务**：图像分类、目标检测、语义分割等大多数任务，尤其适合大规模数据和CV任务。

---

### 2. **Adam**
- **全称**：Adaptive Moment Estimation
- **特点**：自适应学习率，结合Momentum和RMSProp优点，收敛快，调参简单。
- **适用任务**：NLP、CV、生成模型、Transformer、BERT等，适合大多数深度学习任务。

---

### 3. **RMSProp**
- **全称**：Root Mean Square Propagation
- **特点**：自适应调整每个参数的学习率，适合处理非平稳目标。
- **适用任务**：RNN、LSTM等序列建模任务。

---

### 4. **Adagrad**
- **全称**：Adaptive Gradient Algorithm
- **特点**：对稀疏数据友好，学习率随训练逐步减小。
- **适用任务**：NLP中的稀疏特征、推荐系统。

---

### 5. **Adadelta**
- **特点**：改进Adagrad，解决其学习率过快下降问题。
- **适用任务**：通用，适合需要自适应学习率的任务。

---

### 6. **AdamW**
- **全称**：Adam with Weight Decay
- **特点**：在Adam基础上正确实现权重衰减（L2正则），提升泛化能力。
- **适用任务**：Transformer、BERT、ViT等大模型训练。

---

### 7. **Nadam**
- **全称**：Nesterov-accelerated Adam
- **特点**：结合Nesterov动量和Adam，提升收敛速度。
- **适用任务**：通用，适合需要更快收敛的任务。

---

### 8. **LAMB**
- **全称**：Layer-wise Adaptive Moments optimizer for Batch training
- **特点**：适合大批量训练，常用于大规模预训练（如BERT、GPT）。
- **适用任务**：大模型、大批量训练（LLM、BERT等）。

---

**总结表：**

| 优化器   | 适用任务/场景                  |
| -------- | ------------------------------ |
| SGD      | 图像分类、CV大多数任务         |
| Adam     | NLP、CV、生成模型、Transformer |
| RMSProp  | RNN、LSTM、序列建模            |
| Adagrad  | NLP稀疏特征、推荐系统          |
| Adadelta | 通用                           |
| AdamW    | Transformer、BERT、ViT等       |
| Nadam    | 通用，需更快收敛               |
| LAMB     | 大模型、大批量训练             |

---

# 损失函数

深度学习中常用的损失函数及其适用任务如下：

---

### 1. **交叉熵损失（Cross Entropy Loss）**
- **公式**：`-sum(y_true * log(y_pred))`
- **适用任务**：分类（如二分类、多分类、目标检测中的分类头）
- **常用API**：`nn.CrossEntropyLoss`、`nn.BCELoss`、`nn.BCEWithLogitsLoss`

---

### 2. **均方误差损失（MSE Loss）**
- **公式**：`mean((y_true - y_pred)^2)`
- **适用任务**：回归（如房价预测、关键点回归、图像重建）
- **常用API**：`nn.MSELoss`

---

### 3. **平均绝对误差损失（MAE/L1 Loss）**
- **公式**：`mean(|y_true - y_pred|)`
- **适用任务**：回归、对异常值鲁棒的回归任务
- **常用API**：`nn.L1Loss`

---

### 4. **Dice Loss**
- **公式**：`1 - (2 * |A∩B|) / (|A| + |B|)`
- **适用任务**：语义分割、医学图像分割
- **常用API**：`monai.losses.DiceLoss`、自定义

---

### 5. **IoU Loss（交并比损失）**
- **公式**：`1 - IoU(pred, target)`
- **适用任务**：目标检测、分割（关注区域重叠度）
- **常用API**：`torchvision.ops.sigmoid_iou_loss`、自定义

---

### 6. **Focal Loss**
- **作用**：降低易分类样本权重，聚焦难分类样本
- **适用任务**：类别不平衡的分类、目标检测（如RetinaNet）
- **常用API**：`torchvision.ops.sigmoid_focal_loss`、自定义

---

### 7. **KL散度损失（KL Divergence Loss）**
- **公式**：`sum(y_true * log(y_true / y_pred))`
- **适用任务**：概率分布拟合、知识蒸馏、生成模型
- **常用API**：`nn.KLDivLoss`

---

### 8. **CTC Loss（Connectionist Temporal Classification）**
- **适用任务**：序列到序列对齐（如语音识别、OCR）
- **常用API**：`nn.CTCLoss`

---

### 9. **Triplet Loss（三元组损失）**
- **适用任务**：度量学习、检索、人脸识别
- **常用API**：`nn.TripletMarginLoss`

---

### 10. **Hinge Loss（合页损失）**
- **适用任务**：支持向量机（SVM）、部分二分类任务
- **常用API**：`nn.MultiMarginLoss`

---

### 11. **Smooth L1 Loss（Huber Loss）**
- **适用任务**：目标检测回归头、对异常值鲁棒的回归
- **常用API**：`nn.SmoothL1Loss`

---

**总结表：**

| 损失函数          | 适用任务/场景         |
| ----------------- | --------------------- |
| Cross Entropy     | 分类、目标检测分类头  |
| BCE/BCEWithLogits | 二分类、分割          |
| MSE               | 回归、重建            |
| L1/MAE            | 回归、异常值鲁棒      |
| Dice              | 语义分割、医学分割    |
| IoU               | 检测、分割            |
| Focal             | 类别不平衡分类、检测  |
| KLDiv             | 蒸馏、概率分布拟合    |
| CTC               | 序列对齐（语音、OCR） |
| Triplet           | 检索、度量学习        |
| Hinge             | SVM、二分类           |
| Smooth L1         | 检测回归、鲁棒回归    |

---

