------

# æ·±åº¦å­¦ä¹ ä»»åŠ¡ â†’ PyTorch è¾“å…¥ & è¾“å‡ºæ ¼å¼å¯¹ç…§è¡¨

| ä»»åŠ¡ç±»åˆ«                                    | è¾“å…¥æ•°æ®æ ¼å¼                                                 | è¾“å‡ºæ•°æ®æ ¼å¼                                                 | è¯´æ˜ä¸ç¤ºä¾‹                                                   |
| ------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **åºåˆ—åˆ†ç±» (Sequence Classification)**      | `(batch_size, seq_len)` â†’ token ids                          | `(batch_size, num_labels)`                                   | æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ä¾‹ï¼šæƒ…æ„Ÿåˆ†æï¼Œè¾“å‡ºæ­£è´Ÿæƒ…ç»ªçš„æ¦‚ç‡åˆ†å¸ƒã€‚         |
| **Token åˆ†ç±» (Token Classification / NER)** | `(batch_size, seq_len)`                                      | `(batch_size, seq_len, num_labels)`                          | æ¯ä¸ª token ä¸€ä¸ªæ ‡ç­¾ã€‚ä¾‹ï¼šå¥å­ `"John lives in Paris"` â†’ æ¯ä¸ªè¯å¯¹åº”æ ‡ç­¾ã€‚ |
| **å› æœè¯­è¨€å»ºæ¨¡ (Causal LM)**                | `(batch_size, seq_len)`                                      | `(batch_size, seq_len, vocab_size)`                          | è¾“å…¥åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹ï¼š`"The cat sat on the"` â†’ é¢„æµ‹ `"mat"`ã€‚ |
| **æ©ç è¯­è¨€å»ºæ¨¡ (Masked LM)**                | `(batch_size, seq_len)`ï¼Œå« `[MASK]`                         | `(batch_size, seq_len, vocab_size)`                          | é¢„æµ‹è¢« mask çš„ tokenã€‚ä¾‹ï¼š`"The cat sat on the [MASK]"` â†’ è¾“å‡º `"mat"` æ¦‚ç‡æœ€é«˜ã€‚ |
| **åºåˆ—åˆ°åºåˆ— (Seq2Seq LM / ç¿»è¯‘/æ‘˜è¦)**     | Encoder è¾“å…¥ `(batch_size, src_seq_len)`ï¼›Decoder è¾“å…¥ `(batch_size, tgt_seq_len)` | `(batch_size, tgt_seq_len, vocab_size)`                      | ç¿»è¯‘ã€æ‘˜è¦ç­‰ã€‚ä¾‹ï¼šè‹±æ–‡ `"Hello"` â†’ è¾“å‡ºæ³•æ–‡ `"Bonjour"`ã€‚    |
| **é—®ç­” (QA - Extractive)**                  | `(batch_size, seq_len)` (context+question æ‹¼æ¥)              | `(batch_size, seq_len)` start_logits + `(batch_size, seq_len)` end_logits | æ¨¡å‹é¢„æµ‹ç­”æ¡ˆ span çš„èµ·æ­¢ä½ç½®ã€‚ä¾‹ï¼šç­”æ¡ˆ `"Paris"`ã€‚           |
| **å¤šé€‰ä»»åŠ¡ (Multiple Choice)**              | `(batch_size, num_choices, seq_len)`                         | `(batch_size, num_choices)`                                  | æ¯ä¸ªå€™é€‰ä¸€ä¸ªå¾—åˆ†ã€‚ä¾‹ï¼šé—®é¢˜ `"Where is John?"`ï¼Œé€‰é¡¹ ["London", "Paris"] â†’ Paris åˆ†æ•°é«˜ã€‚ |
| **å›¾åƒåˆ†ç±»**                                | `(batch_size, channels, height, width)`                      | `(batch_size, num_classes)`                                  | å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚ä¾‹ï¼šCIFAR-10 å›¾åƒ â†’ è¾“å‡º 10 ç±»æ¦‚ç‡åˆ†å¸ƒã€‚       |
| **ç›®æ ‡æ£€æµ‹ (Detection)**                    | å›¾åƒ `(B, C, H, W)`ï¼›æ ‡ç­¾ `List[Dict]`                       | è®­ç»ƒï¼šLoss dictï¼›æ¨ç†ï¼šList[Dict]ï¼Œæ¯ä¸ªåŒ…å« `boxes (N,4)`, `labels (N,)`, `scores (N,)` | æ¯å¼ å›¾åƒè¾“å‡ºè‹¥å¹²æ£€æµ‹æ¡†å’Œç±»åˆ«ã€‚                               |
| **å›¾åƒåˆ†å‰² (Segmentation)**                 | å›¾åƒ `(B, C, H, W)`ï¼›mask `(B, H, W)`                        | `(B, num_classes, H, W)`                                     | æ¯ä¸ªåƒç´ ä¸€ä¸ªç±»åˆ«æ¦‚ç‡ã€‚                                       |
| **è¯­éŸ³åˆ†ç±» (Audio Classification)**         | `(batch_size, time_steps)` æˆ– `(batch_size, channels, time_steps)` | `(batch_size, num_classes)`                                  | è¾“å…¥æ³¢å½¢ â†’ è¾“å‡ºç±»åˆ«åˆ†å¸ƒã€‚                                    |
| **è¯­éŸ³è¯†åˆ« (ASR - CTC)**                    | `(batch_size, time_steps, features)`                         | `(batch_size, time_steps, vocab_size)`                       | CTC è¾“å‡ºæ¯å¸§ token åˆ†å¸ƒï¼Œç»è¿‡è§£ç å¾—åˆ°æ–‡å­—ã€‚                  |
| **è¡¨æ ¼é—®ç­” (Table QA)**                     | `(batch_size, seq_len)` (è¡¨æ ¼è½¬åºåˆ—)                         | `(batch_size, seq_len)` æˆ– `(batch_size, num_labels)`        | è¾“å‡ºå¯èƒ½æ˜¯ç­”æ¡ˆ span æˆ–åˆ†ç±»ç»“æœã€‚                             |
| **è§†è§‰é—®ç­” (VQA)**                          | å›¾åƒ `(B, C, H, W)` + é—®é¢˜ `(B, seq_len)`                    | `(batch_size, num_answers)` æˆ– `(batch_size, vocab_size)`    | å¤šæ¨¡æ€è¾“å…¥ â†’ è¾“å‡ºç­”æ¡ˆç±»åˆ«æˆ–ç”Ÿæˆç­”æ¡ˆæ–‡æœ¬ã€‚                    |

------

è¿™æ ·ä½ å°±èƒ½ä¸€çœ¼çœ‹åˆ°ï¼š

- è¾“å…¥æ•°æ®å¼ é‡æ€ä¹ˆç»„ç»‡
- è¾“å‡ºä¸€èˆ¬æ˜¯ä»€ä¹ˆå½¢çŠ¶

ğŸ‘‰ åœ¨è®­ç»ƒæ—¶ï¼Œè¾“å‡ºé€šå¸¸æ˜¯ **logits**ï¼ˆæœªç»è¿‡ softmaxï¼‰ï¼Œæ–¹ä¾¿ç”¨ `CrossEntropyLoss` ç­‰ loss functionã€‚



# ä¸¾å‡ ä¸ªä»£ç ä¾‹å­

### 1. æ–‡æœ¬åˆ†ç±» (SequenceClassification)

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

inputs = tokenizer(["I love this movie!", "This film is terrible..."],
                   padding=True, truncation=True, return_tensors="pt")

outputs = model(**inputs)  # inputs['input_ids'] shape: (batch_size, seq_len)
```

### 2. å›¾åƒåˆ†ç±»

```python
import torch
from torchvision import models

model = models.resnet18(pretrained=True)
x = torch.randn(16, 3, 224, 224)  # batch=16
y = model(x)  # è¾“å‡º shape: (16, num_classes)
```

### 3. ç›®æ ‡æ£€æµ‹ (FasterRCNN)

```python
from torchvision.models.detection import fasterrcnn_resnet50_fpn
import torch

model = fasterrcnn_resnet50_fpn(pretrained=True)
images = [torch.randn(3, 224, 224), torch.randn(3, 300, 300)]  # list of images
targets = [
    {"boxes": torch.tensor([[50, 50, 150, 150]]), "labels": torch.tensor([1])},
    {"boxes": torch.tensor([[30, 40, 100, 200]]), "labels": torch.tensor([2])}
]
output = model(images, targets)
```

------





