如果一个模型效果不好，通常有**四大类可调节的方向**：**数据 → 模型 → 训练 → 推理/评估**。我帮你总结成一个规律性框架，你以后排查模型问题就可以按这个逻辑走。

------

# 一、数据层面（最优先考虑）

- **数据质量**：标签错、噪声多 → 清洗 / 重标注。
- **数据量**：样本太少 → 增加样本、数据增强（图像翻转、文本同义替换）。
- **数据分布**：训练集和验证集分布不一致 → 检查采样方式 / 收集更接近真实场景的数据。
- **类别不平衡**：多数类压制少数类 → 重采样 / 类别权重 / Focal Loss。

📌 **规律：Garbage In, Garbage Out —— 输入不靠谱，再好的模型也学不出好结果。**

------

# 二、模型结构层面

- **容量不足**：模型太小 → 增加层数/宽度、换更强架构。
- **容量过大**：过拟合 → 降低参数量 / 加正则化（Dropout, L2）。
- **特征表达能力不足**：换合适的架构：
  - 序列 → LSTM/Transformer
  - 图像 → CNN/Vision Transformer
  - 表格 → MLP + 特征工程 / GBDT

📌 **规律：模型要“刚刚好”，太弱学不动，太强容易记忆而不泛化。**

------

# 三、训练策略层面

- **学习率**：过大震荡不收敛 / 过小收敛太慢。→ 调整 LR / 使用 scheduler。
- **优化器**：SGD vs AdamW → 不同任务效果差异大。
- **批大小（batch size）**：过小噪声大，过大泛化差。
- **正则化**：Dropout, Weight Decay, 数据增强。
- **初始化**：不合理初始化可能陷入坏局部极值 → 用预训练权重更稳。

📌 **规律：训练像“开车”，学习率是油门，正则是刹车，要调到合适的平衡。**

------

# 四、推理 & 评估层面

- **评估指标不合理**：仅看准确率但数据不平衡 → 应该用 F1/ROC-AUC。
- **推理时处理不一致**：训练用归一化 / padding，推理忘了用。
- **过拟合的假象**：训练集高，验证集低 → 说明模型没泛化。
- **任务定义不清**：是不是在用分类模型做排序任务？是不是标签本身有歧义？

📌 **规律：别只盯着“训练 loss”，要看验证集/测试集是否真的提升。**

------

# 五、一个排查口诀

👉 **“数模训评”四步走**

1. **数**：先看数据有没有问题。
2. **模**：再看模型容量和结构是否匹配任务。
3. **训**：调学习率、优化器、正则化。
4. **评**：确保评估指标与任务目标一致。





---

# 模型效果不好排查对照表

| **症状表现**                                   | **可能原因**                                                 | **调整思路 / 对策**                                          |
| ---------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **训练和验证都差**（loss 高，acc/F1 低）       | - 数据噪声大 / 标签错误<br>- 数据量太少，模型没学到模式<br>- 模型太小，容量不足<br>- 学习率过大，训练发散 | - 清洗数据、重标注<br>- 收集更多样本或做数据增强<br>- 换更大/更强模型结构<br>- 降低学习率，调优化器 |
| **训练好，验证差**（过拟合）                   | - 模型太复杂<br>- 数据量不足<br>- 数据分布不一致（train/test domain shift）<br>- 正则化不足 | - 降低模型规模<br>- 增加正则化（Dropout, Weight Decay）<br>- 数据增强 / 扩充验证集<br>- 迁移学习（预训练模型） |
| **训练差，验证也差，但 loss 不下降**（欠拟合） | - 模型表达能力太弱<br>- 学习率过低，梯度更新慢<br>- 特征选择不足 | - 增加模型深度/宽度<br>- 提高学习率或加 LR scheduler<br>- 加入更多有效特征 |
| **训练 loss 一直抖动，收敛不上去**             | - 学习率过大<br>- 批大小太小，噪声大<br>- 优化器设置不当     | - 调低学习率<br>- 增大 batch size<br>- 换优化器（AdamW → SGD/Lookahead 等） |
| **验证集表现忽高忽低，不稳定**                 | - 数据量太少，验证集随机性强<br>- 过度依赖小验证集<br>- 模型本身不稳定 | - 用交叉验证（K-fold）<br>- 增大验证集规模<br>- 多次实验取平均 |
| **模型对少数类识别差**（类别不平衡）           | - 多数类压制少数类<br>- 损失函数没考虑权重                   | - 过采样少数类 / 欠采样多数类<br>- 损失函数加权（class weight, focal loss）<br>- 采样策略（WeightedRandomSampler） |
| **评估指标看起来高，但真实应用差**             | - 只看准确率，忽略不平衡<br>- 指标选择不合理<br>- 训练/推理处理不一致 | - 换指标：F1、ROC-AUC、Precision@K<br>- 确认推理阶段做了归一化/分词等预处理<br>- 加上业务相关指标（召回率、覆盖率） |
| **验证集 OK，但上线表现差**                    | - 训练/验证数据分布和真实场景差异大<br>- 数据漂移（数据随时间变化） | - 收集线上数据做增量训练<br>- Domain Adaptation（领域自适应）<br>- 持续监控模型表现 |

---

# 调参优先级规律
1. **先看数据**：是不是脏、是不是不平衡。  
2. **再看模型**：太小 → 换大；太大 → 加正则。  
3. **再调训练**：学习率、优化器、batch size。  
4. **最后看评估**：指标是否合理，场景是否匹配。  

---

📌 **一句话口诀**：  
**“数模训评四步走，先保数据别出错；模型容量要适配，训练策略细调优；指标选对看业务。”**

---

