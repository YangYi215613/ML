# Transformerï¼ˆè¶…è¯¦å°½ç¬”è®°ï¼‰â€”â€”æ¶æ„ã€æ¯ä¸€éƒ¨åˆ†çš„ä½œç”¨ã€ä¸ºäº†è§£å†³ä»€ä¹ˆé—®é¢˜ï¼ˆä¾¿äºèƒŒè¯µï¼‰

ä¸‹é¢æ˜¯ä¸€ä»½ç»“æ„åŒ–ä¸”å¯èƒŒè¯µçš„ Transformer æ·±å…¥è¯´æ˜ï¼ŒåŒ…å«æ•°å­¦å…¬å¼ã€å…³é”®å®ç°ç»†èŠ‚ã€å¸¸è§å˜ä½“ä¸å·¥ç¨‹æ³¨æ„ç‚¹ã€‚é¢è¯•æ—¶æŠŠå…³é”®å¥å­èƒŒç†Ÿï¼ˆæœ€åæœ‰é€ŸæŸ¥è¡¨ï¼‰ï¼Œéœ€è¦ä»£ç ç¤ºä¾‹ä¹Ÿæœ‰æœ€å° PyTorch å®ç°ç‰‡æ®µå¯ä»¥ç°åœºè®²è§£ã€‚

------

# 1. é«˜å±‚æ¦‚è§ˆï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰

Transformerï¼ˆVaswani et al., 2017ï¼‰æŠŠ**æ³¨æ„åŠ›ï¼ˆAttentionï¼‰**æ”¾åœ¨ä¸­å¿ƒï¼Œå–ä»£äº† RNN/CNN åœ¨åºåˆ—å»ºæ¨¡ä¸­çš„é¡ºåºè®¡ç®—ï¼Œä½¿æ¨¡å‹å¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—ã€æ•è·é•¿è·ç¦»ä¾èµ–ï¼Œä»è€Œè§£å†³äº† RNN çš„é•¿è·ç¦»ä¾èµ–è¡°å‡ä¸ä¸²è¡ŒåŒ–ç“¶é¢ˆé—®é¢˜ã€‚

------

# 2. ä¸‰ç§å¸¸è§çš„ Transformer æ¨¡å¼ï¼ˆç”¨é€”ä¸å·®åˆ«ï¼‰

- **Encoder-Decoderï¼ˆåŸå§‹ Transformer / T5ï¼‰**ï¼šé€‚åˆåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆæœºå™¨ç¿»è¯‘ã€æ‘˜è¦ï¼‰ã€‚Encoder ç¼–ç  sourceï¼ŒDecoder è‡ªå›å½’ç”Ÿæˆ targetï¼Œå¹¶é€šè¿‡ cross-attention è®¿é—® encoder è¾“å‡ºã€‚
- **Encoder-onlyï¼ˆBERTï¼‰**ï¼šç”¨äºç†è§£ç±»ä»»åŠ¡ï¼ˆåˆ†ç±»ã€é—®ç­”æ£€ç´¢ã€å¥å­è¡¨ç¤ºï¼‰ï¼Œè®­ç»ƒç›®æ ‡æ˜¯ Masked LM æˆ–ä¸‹ä¸€å¥åˆ¤æ–­ç­‰ã€‚
- **Decoder-onlyï¼ˆGPTï¼‰**ï¼šç”¨äºç”Ÿæˆç±»ä»»åŠ¡ï¼ˆæ–‡æœ¬ç»­å†™ã€å¯¹è¯ï¼‰ï¼ŒåŸºäºè‡ªå›å½’ï¼ˆcausalï¼‰è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚

------

# 3. æ•´ä½“æ¨¡å—å›¾ï¼ˆæ¯å±‚/æ¯å—çš„é¡ºåºï¼‰

å•ä¸ª Encoder å±‚ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ï¼š

1. Input Embedding + Positional Encoding
2. Multi-Head Self-Attention (MHSA)
3. Add & LayerNorm
4. Position-wise Feed-Forward Network (FFN)
5. Add & LayerNorm

å•ä¸ª Decoder å±‚ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ï¼š

1. Input Embedding + Positional Encoding
2. Masked Multi-Head Self-Attentionï¼ˆè‡ªå›å½’ï¼Œå±è”½æœªæ¥ï¼‰
3. Add & LayerNorm
4. Encoder-Decoder Cross-Attentionï¼ˆæŸ¥è¯¢ decoder, key/value æ¥è‡ª encoderï¼‰
5. Add & LayerNorm
6. FFN
7. Add & LayerNorm

------

# 4. å…³é”®ç»„ä»¶é€ä¸€æ‹†è§£ï¼ˆå«å…¬å¼ã€å½¢çŠ¶ä¸â€œä¸ºäº†è§£å†³ä»€ä¹ˆâ€ï¼‰

## 4.1 Embedding + Positional Encoding

- **åšä»€ä¹ˆ**ï¼šæŠŠç¦»æ•£ token è½¬ä¸ºå‘é‡ï¼ˆembeddingï¼‰ï¼Œå¹¶æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼ˆTransformer æ— å·ç§¯/é€’å½’ï¼Œå¿…é¡»æ˜¾å¼æä¾›åºåˆ—é¡ºåºï¼‰ã€‚

- **å¸¸è§å®ç°**ï¼š

  - å¯å­¦ä¹ ä½ç½®å‘é‡ï¼ˆlearned positional embeddingsï¼‰ã€‚

  - æˆ–æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆsin/cosï¼‰ï¼š

    PEpos,2i=sinâ¡(pos100002i/dmodel),PEpos,2i+1=cosâ¡(pos100002i/dmodel)\text{PE}_{pos,2i} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right),\quad \text{PE}_{pos,2i+1} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)

- **ä¸ºäº†è§£å†³ä»€ä¹ˆ**ï¼šæä¾› token çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿æ¨¡å‹èƒ½åŒºåˆ†â€œåœ¨åºåˆ—ä¸­å“ªé‡Œâ€ã€‚

## 4.2 Scaled Dot-Product Attentionï¼ˆæ ‡é‡ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰

- **è¾“å…¥/è¾“å‡º**ï¼šQï¼ˆqueriesï¼‰ã€Kï¼ˆkeysï¼‰ã€Vï¼ˆvaluesï¼‰ã€‚

- **å…¬å¼**ï¼š

  Attention(Q,K,V)=softmaxâ€‰â£(QKâŠ¤dk)V\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V

- **mask**ï¼šå¯åœ¨ softmax ä¹‹å‰æŠŠä¸å…è®¸çš„ä½ç½®ç½®ä¸º âˆ’âˆï¼ˆç”¨äºå¡«å…… mask æˆ– decoder çš„ causal maskï¼‰ã€‚

- **å½¢çŠ¶**ï¼ˆå¸¸è§ï¼‰ï¼š

  - Q: (B, T_q, d_k), K,V: (B, T_k, d_k)/(B, T_k, d_v) â†’ è¾“å‡º: (B, T_q, d_v)

- **ä¸ºäº†è§£å†³ä»€ä¹ˆ**ï¼šå…è®¸æ¨¡å‹å¯¹åºåˆ—ä¸­ä»»æ„ä½ç½®å»ºç«‹ç›´æ¥çš„ç›¸äº’å½±å“ï¼ˆé•¿è·ç¦»ä¾èµ–ï¼‰ï¼Œå¹¶ä¸”å¯å¹¶è¡Œè®¡ç®—ï¼ˆç›¸å¯¹äº RNN ä¸²è¡Œï¼‰ã€‚

## 4.3 Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

- **åšä»€ä¹ˆ**ï¼šæŠŠ embedding æŠ•å½±åˆ°å¤šä¸ªå­ç©ºé—´ï¼ˆheadsï¼‰ï¼Œåœ¨ä¸åŒå­ç©ºé—´åˆ†åˆ«åšæ³¨æ„åŠ›ï¼Œæœ€åæ‹¼å›ï¼š

  headi=Attention(QWiQ,KWiK,VWiV)\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)

  MultiHead(Q,K,V)=Concat(head1,...,headh)WO\text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,...,\text{head}_h)W^O

- **å‚æ•°**ï¼š

  - d_modelï¼šæ€» embedding ç»´åº¦ï¼Œhï¼šhead æ•°ï¼Œd_k = d_{model}/hã€‚

- **ä¸ºäº†è§£å†³ä»€ä¹ˆ**ï¼šä¸åŒ head å¯ä»¥å­¦ä¹ ä¸åŒçš„å…³ç³»ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€çŸ­/é•¿è·ç¦»ä¾èµ–ï¼‰ï¼Œæå‡è¡¨ç¤ºèƒ½åŠ›å¹¶ç¨³å®šå­¦ä¹ ã€‚

## 4.4 Residual Connectionï¼ˆæ®‹å·®ï¼‰ + LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰

- **åšä»€ä¹ˆ**ï¼šåœ¨å­å±‚ï¼ˆAttention / FFNï¼‰ååŠ æ®‹å·®å¹¶åšå½’ä¸€åŒ–ï¼š

  output=LayerNorm(x+Sublayer(x))\text{output} = \text{LayerNorm}(x + \text{Sublayer}(x))

- **ä¸ºäº†è§£å†³ä»€ä¹ˆ**ï¼šæ®‹å·®ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€ä¿ƒè¿›è®­ç»ƒæ·±å±‚ç½‘ç»œï¼›LayerNorm ç¨³å®šè®­ç»ƒã€åŠ é€Ÿæ”¶æ•›ã€‚æ³¨æ„ï¼šæœ‰ä¸¤ç§å†™æ³• post-normï¼ˆåŸæ–‡ï¼‰å’Œ pre-normï¼ˆæ›´ç¨³å®šæ›´å¸¸ç”¨äºæ·±æ¨¡å‹ï¼‰ã€‚

## 4.5 Position-wise Feed-Forward Networkï¼ˆé€ä½ç½®å‰é¦ˆç½‘ç»œï¼‰

- **ç»“æ„**ï¼šä¸¤å±‚å…¨è¿æ¥ + éçº¿æ€§ï¼ˆå¸¸ç”¨ GELU/ReLUï¼‰ï¼š

  FFN(x)=Linear2(GELU(Linear1(x)))\text{FFN}(x) = \text{Linear}_2(\text{GELU}(\text{Linear}_1(x)))

  é€šå¸¸ Linear_1 ä¸º d_model â†’ d_ffï¼ˆå¦‚ 4*d_modelï¼‰ï¼Œç„¶åå›åˆ° d_modelã€‚

- **ä¸ºäº†è§£å†³ä»€ä¹ˆ**ï¼šåŠ å…¥éçº¿æ€§ä¸è·¨é€šé“å˜æ¢ï¼ˆåœ¨æ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†ï¼Œè¡¥å……æ³¨æ„åŠ›çš„â€œä½ç½®é—´â€ä½œç”¨ï¼‰ã€‚

------

# 5. Decoder çš„ç‰¹æ®Šæœºåˆ¶ï¼ˆè‡ªå›å½’ä¸ cross-attentionï¼‰

- **Masked Self-Attention**ï¼šdecoder çš„è‡ªæ³¨æ„åŠ›ç”¨ causal maskï¼Œç¡®ä¿ç”Ÿæˆæ—¶ä¸èƒ½çœ‹åˆ°æœªæ¥ tokenï¼ˆå®ç°è‡ªå›å½’ï¼‰ã€‚
- **Cross-Attention**ï¼šdecoder çš„ç¬¬äºŒä¸ªæ³¨æ„åŠ›å±‚ä»¥ decoder çš„ Q å’Œ encoder çš„ K,V ä¸ºè¾“å…¥ï¼ŒæŠŠ source ä¿¡æ¯å¼•å…¥ç”Ÿæˆè¿‡ç¨‹ï¼ˆseq2seq çš„å¯¹é½æœºåˆ¶ï¼‰ã€‚
- **ä¸ºäº†è§£å†³ä»€ä¹ˆ**ï¼šMasked ä¿è¯æ­£ç¡®çš„æ¦‚ç‡åˆ†è§£ï¼›Cross-Attention å®ç°æºâ€”ç›®æ ‡ä¿¡æ¯æ¡¥æ¥ï¼ˆå¦‚ç¿»è¯‘ä¸­çš„å¯¹é½ï¼‰ã€‚

------

# 6. ç®—æ³•å¤æ‚åº¦ä¸ç“¶é¢ˆï¼ˆå·¥ç¨‹è§†è§’ï¼‰

- **è‡ªæ³¨æ„åŠ›çš„æ—¶é—´/ç©ºé—´å¤æ‚åº¦**ï¼š
  - æ—¶é—´ï¼š O(n2â‹…d)O(n^2 \cdot d)ï¼ˆä¸»è¦æ˜¯ QK^T çš„ O(n^2 d_k)ï¼‰ï¼›
  - ç©ºé—´ï¼š O(n2)O(n^2)ï¼ˆéœ€è¦å­˜å‚¨æ³¨æ„åŠ›çŸ©é˜µï¼‰ã€‚
     è¿™é‡Œ n=åºåˆ—é•¿åº¦ï¼Œd=æ¨¡å‹ç»´åº¦ã€‚
- **FFN å¤æ‚åº¦**ï¼š O(nâ‹…dâ‹…dff)O(n \cdot d \cdot d_{ff})ï¼ˆå¸¸ d_{ff}â‰ˆ4dï¼Œå› è€Œçº¦ä¸º O(n d^2)ï¼‰ã€‚
- **ç“¶é¢ˆ**ï¼šå½“ n å¾ˆå¤§æ—¶ï¼ˆé•¿æ–‡æœ¬/é•¿åºåˆ—ï¼‰ï¼Œè‡ªæ³¨æ„åŠ›çš„ n^2 æˆæœ¬æˆä¸ºä¸»å¯¼ã€‚
- **å¸¸è§è§£å†³æ–¹æ³•**ï¼š
  - å±€éƒ¨/ç¨€ç–æ³¨æ„åŠ›ï¼ˆLongformerã€BigBirdï¼‰
  - ä½ç§©/è¿‘ä¼¼æ³¨æ„åŠ›ï¼ˆLinformerã€Performerï¼‰
  - å¯ç¼“å­˜ Key/Valueï¼ˆTransformer-XLï¼Œæå‡è·¨ç¯‡ç« è®°å¿†å¹¶å‡å°‘é‡å¤è®¡ç®—ï¼‰
  - åˆ†å±‚/æ»‘åŠ¨çª—å£ã€æ£€ç´¢å¢å¼ºï¼ˆRAGï¼‰ç­‰

------

# 7. è®­ç»ƒç›®æ ‡ä¸å¾®è°ƒç­–ç•¥

- **åŸå§‹ Seq2Seqï¼ˆencoder-decoderï¼‰**ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰è®­ç»ƒ target åºåˆ—ï¼ˆteacher forcingï¼‰ã€‚
- **Encoder-onlyï¼ˆBERTï¼‰**ï¼šMasked Language Modelingï¼ˆMLMï¼‰+ NSP/ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒã€‚
- **Decoder-onlyï¼ˆGPTï¼‰**ï¼šCausal LMï¼ˆè‡ªå›å½’ï¼‰è®­ç»ƒã€‚
- **å¾®è°ƒæŠ€å·§**ï¼š
  - å…¨å‚æ•°å¾®è°ƒ
  - å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼šAdaptersã€LoRAã€Prefix-tuningã€Prompt-tuningï¼ˆå‚æ•°å°‘ã€é€Ÿåº¦å¿«ï¼‰
  - æŒ‡ä»¤å¾®è°ƒ / RLHFï¼ˆä½¿æ¨¡å‹æ›´ç¬¦åˆäººç±»åå¥½ä¸å®‰å…¨çº¦æŸï¼‰

------

# 8. å®é™…å®ç°ç»†èŠ‚ä¸è¶…å‚å»ºè®®

- **å…¸å‹è¶…å‚**ï¼š
  - baseï¼šd_model=512, heads=8, d_ff=2048, layers=6
  - largeï¼šd_model=1024, heads=16, d_ff=4096, layers=24
- **ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦**ï¼š
  - Adam / AdamWï¼Œwarmup + inverse sqrt æˆ– cosine decay å¸¸ç”¨ï¼ˆwarmup å¯ç¨³å®šåˆæœŸè®­ç»ƒï¼‰ã€‚
- **æ­£åˆ™åŒ–**ï¼šdropoutï¼ˆattention & ffnï¼‰ã€label smoothingã€‚
- **è®­ç»ƒå·¥ç¨‹**ï¼š
  - æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰å‡å°‘æ˜¾å­˜ä¸åŠ é€Ÿï¼›
  - æ¢¯åº¦ç´¯ç§¯ï¼ˆå°æ˜¾å­˜ä¸‹æ¨¡æ‹Ÿå¤§ batchï¼‰ï¼›
  - æ¨¡å‹å¹¶è¡Œ / å¼ é‡å¹¶è¡Œ + pipeline å¹¶è¡Œï¼ˆè®­ç»ƒå¤§æ¨¡å‹ï¼‰ï¼›
  - gradient checkpointingï¼ˆèŠ‚çœå†…å­˜ï¼Œç‰ºç‰²ä¸€å®šè®¡ç®—ï¼‰ã€‚

------

# 9. å¸¸è§å˜ä½“é€Ÿè§ˆï¼ˆä¸€å¥è¯è¯´æ˜ï¼‰

- **Transformer-XL**ï¼šå¼•å…¥ç›¸å¯¹ä½ç½®ä¸å¯ç¼“å­˜è®°å¿†ï¼Œæ”¹å–„é•¿åºåˆ—ä¾èµ–ã€‚
- **Reformer**ï¼šç”¨ LSH attention & reversible layers æ¥é™ä½å†…å­˜/æ—¶é—´ã€‚
- **Longformer / BigBird**ï¼šç¨€ç–+å…¨å±€æ³¨æ„åŠ›ï¼Œé€‚åˆè¶…é•¿æ–‡æœ¬ã€‚
- **Performer**ï¼šç”¨éšæœºç‰¹å¾è¿‘ä¼¼ softmax-attentionï¼ˆçº¿æ€§æ³¨æ„ï¼‰ã€‚
- **T5**ï¼šç»Ÿä¸€ textâ†’text æ¡†æ¶ï¼ˆencoder-decoderï¼‰ã€‚
- **GPT ç³»åˆ—**ï¼šdecoder-onlyï¼Œè‡ªå›å½’å¤§æ¨¡å‹ã€‚
- **Adapter/LoRA**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚

------

# 10. å¸¸è§é¢è¯•/è®²è§£ç‚¹ï¼ˆè¦èƒ½èƒŒè¯µçš„å¥å­ï¼‰

- â€œAttention çš„å…¬å¼æ˜¯ softmax(QKáµ€ / âˆšd_k) Vï¼Œè¿™æ˜¯ Transformer çš„æ ¸å¿ƒï¼Œå®ƒèƒ½ç›´æ¥å»ºæ¨¡ä»»æ„ä½ç½®é—´çš„ä¾èµ–ã€‚â€
- â€œMulti-head é€šè¿‡å¹¶è¡Œå­ç©ºé—´æ•è·å¤šç±»å…³ç³»ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›ä¸ç¨³å®šæ€§ã€‚â€
- â€œæ®‹å·® + LayerNorm è§£å†³æ·±å±‚ç½‘ç»œçš„è®­ç»ƒå›°éš¾ï¼Œpre-norm å†™æ³•åœ¨æ·±å±‚æ›´ç¨³å®šã€‚â€
- â€œTransformer å¹¶è¡ŒåŒ–èƒ½åŠ›å¼ºï¼ˆè·¨åºåˆ—å¹¶è¡Œï¼‰ï¼Œæ¯” RNN æ›´å®¹æ˜“åˆ©ç”¨ç°ä»£ç¡¬ä»¶åŠ é€Ÿã€‚â€
- â€œè‡ªæ³¨æ„åŠ›çš„ nÂ² æˆæœ¬æ˜¯ç“¶é¢ˆï¼Œé•¿åºåˆ—é€šå¸¸é€šè¿‡ç¨€ç–/çº¿æ€§/åˆ†å±‚/æ£€ç´¢ç­‰æ–¹å¼è§£å†³ã€‚â€

------

# 11. æœ€å°å¯è¯» PyTorch ä»£ç ç‰‡æ®µï¼ˆä¾¿äºé¢è¯•ç°åœºè®²è§£ï¼‰

```python
# æ³¨æ„ï¼šè¿™æ˜¯æ•™å­¦ç‰ˆç®€åŒ–å®ç°ï¼Œå±•ç¤º Shapes & å…³é”®æ­¥éª¤
import torch, math
import torch.nn as nn
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    # Q: (B, h, T_q, d_k), K,V: (B, h, T_k, d_k)
    scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(Q.size(-1))  # (B,h,T_q,T_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-1e9'))
    attn = F.softmax(scores, dim=-1)
    out = torch.matmul(attn, V)  # (B,h,T_q,d_k)
    return out, attn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.h = num_heads
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        B, T, _ = x.size()
        # project and reshape to (B, h, T, d_k)
        Q = self.w_q(x).view(B, T, self.h, self.d_k).transpose(1,2)
        K = self.w_k(x).view(B, T, self.h, self.d_k).transpose(1,2)
        V = self.w_v(x).view(B, T, self.h, self.d_k).transpose(1,2)
        out, attn = scaled_dot_product_attention(Q, K, V, mask)
        out = out.transpose(1,2).contiguous().view(B, T, -1)  # (B, T, d_model)
        return self.out_proj(out)

class TransformerBlock(nn.Module):
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, nhead)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model))
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        attn_out = self.mha(x, mask)
        x = self.norm1(x + self.dropout(attn_out))
        ffn_out = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_out))
        return x
```

ä½ å¯ä»¥ç°åœºè®²è§£æ¯ä¸€è¡Œï¼šä¸ºä»€ä¹ˆè¦ .view + transposeï¼ˆåˆ†å¤´ï¼‰ï¼Œä¸ºä½•é™¤ä»¥ sqrt(d_k)ï¼ˆæ§åˆ¶ softmax æ¢¯åº¦å°ºåº¦ï¼‰ï¼Œä¸ºä»€ä¹ˆæ®‹å·®å LayerNormï¼ˆç¨³å®šè®­ç»ƒï¼‰ç­‰ã€‚

------

# 12. è°ƒè¯•ä¸å¸¸è§é—®é¢˜ï¼ˆé¢è¯•å¿…ä¼šï¼‰

- **NaN / æ¢¯åº¦çˆ†ç‚¸**ï¼šæ£€æŸ¥å­¦ä¹ ç‡ã€warmupã€LayerNorm ä¸åˆå§‹åŒ–ï¼›å¸¸ä½¿ç”¨ AdamW + warmup solvesã€‚
- **mask æ–¹å‘é”™è¯¯**ï¼šç”Ÿæˆä»»åŠ¡ä¼šå› ä¸º mask é”™ä½æ³„éœ²æœªæ¥ä¿¡æ¯ï¼Œå¯¼è‡´è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´ã€‚
- **å½¢çŠ¶é”™è¯¯**ï¼šå¤šå¤´ reshape/transpose æ˜¯å¸¸è§æŠ¥é”™ç‚¹ï¼ˆç»´åº¦é¡ºåºéœ€ç•™å¿ƒï¼‰ã€‚
- **æ€§èƒ½é—®é¢˜**ï¼šCPU è®­ç»ƒä¼šæ…¢ï¼Œæ³¨æ„ç”¨æ··åˆç²¾åº¦ + å¤§ batch æˆ–åˆ†å¸ƒå¼è®­ç»ƒã€‚

------

# 13. é¢è¯•é€ŸæŸ¥è¡¨ï¼ˆèƒŒè¯µè¦ç‚¹ï¼‰

- Attention å…¬å¼ï¼šsoftmax(QKáµ€ / âˆšd_k) Vã€‚
- Multi-headï¼šå¹¶è¡Œå¤šä¸ª attentionï¼Œæœ€åçº¿æ€§å˜æ¢ã€‚
- Positional Encodingï¼šsin/cos æˆ– learnableï¼Œæä¾›é¡ºåºä¿¡æ¯ã€‚
- Blockï¼šMHA â†’ Add+Norm â†’ FFN â†’ Add+Normã€‚
- Decoder ç‰¹æ®Šç‚¹ï¼šmasked self-attn + cross-attnã€‚
- å¤æ‚åº¦ç“¶é¢ˆï¼šattention O(nÂ²)ï¼Œè§£å†³ç­–ç•¥ï¼šç¨€ç–/çº¿æ€§/ç¼“å­˜/æ£€ç´¢ã€‚
- å¸¸ç”¨è¶…å‚ï¼šbase(d=512,h=8,ff=2048,L=6)ï¼Œlarge(d=1024,h=16,...)

------



å¤ªå¥½äº† ğŸ‘ï¼Œé‚£æˆ‘å¸®ä½ æŠŠ **Transformer çŸ¥è¯†ç‚¹**æµ“ç¼©æˆä¸€ä»½ **ä¸€é¡µèƒŒè¯µè®°å¿†å¡**ï¼Œä½ åªè¦è®°ä½é‡Œé¢çš„è¦ç‚¹ï¼Œé¢è¯•æ—¶å°±èƒ½æµç•…å›ç­”ã€‚

------

# ğŸ“Œ Transformer è®°å¿†å¡ï¼ˆé¢è¯•é€ŸèƒŒç‰ˆï¼‰

## 1. æ ¸å¿ƒæ€æƒ³

- **Attention æœºåˆ¶**å–ä»£ RNN/CNN é¡ºåºè®¡ç®—
- å¹¶è¡ŒåŒ–å¤„ç†åºåˆ—ï¼Œæ•è· **é•¿è·ç¦»ä¾èµ–**
- ä¸‰ç§æ¨¡å¼ï¼š
  - Encoder-Decoderï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰
  - Encoder-onlyï¼ˆBERTï¼Œç†è§£ä»»åŠ¡ï¼‰
  - Decoder-onlyï¼ˆGPTï¼Œç”Ÿæˆä»»åŠ¡ï¼‰

------

## 2. æ¨¡å—ç»“æ„

**Encoder å±‚**ï¼š

1. Embedding + Positional Encoding
2. Multi-Head Self-Attention
3. Add & LayerNorm
4. Feed Forward Network (FFN)
5. Add & LayerNorm

**Decoder å±‚**ï¼š

1. Masked Multi-Head Self-Attention
2. Add & LayerNorm
3. Cross-Attentionï¼ˆQ=decoder, K/V=encoderï¼‰
4. Add & LayerNorm
5. FFN
6. Add & LayerNorm

------

## 3. Attention å…¬å¼

$$Attention(Q,K,V)=softmaxâ€‰â£(QKâŠ¤dk)V\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)$$

- Q: Queryï¼ˆè¦æ‰¾ä»€ä¹ˆï¼‰
- K: Keyï¼ˆå€™é€‰ä½ç½®ï¼‰
- V: Valueï¼ˆä¿¡æ¯å†…å®¹ï¼‰
- **âˆšd_k**ï¼šé¿å…ç‚¹ç§¯è¿‡å¤§ï¼Œç¨³å®š softmax

------

## 4. Multi-Head Attention

- å¤šå¤´å¹¶è¡Œï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒå…³ç³»
- æ‹¼æ¥åçº¿æ€§å˜æ¢
- **å¥½å¤„**ï¼šå¢å¼ºè¡¨è¾¾åŠ›ï¼Œæ•è·è¯­æ³•/è¯­ä¹‰/è¿œè¿‘ä¾èµ–

------

## 5. å…¶å®ƒå…³é”®æ¨¡å—

- **Positional Encoding**ï¼šæä¾›ä½ç½®ä¿¡æ¯ï¼ˆsin/cos æˆ–å¯å­¦ä¹ ï¼‰
- **Residual + LayerNorm**ï¼šç¨³å®šè®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- **FFN**ï¼šé€ä½ç½®ä¸¤å±‚å…¨è¿æ¥ï¼Œè¡¥å……éçº¿æ€§

------

## 6. Decoder ç‰¹æ®Šç‚¹

- **Masked Self-Attention**ï¼šå±è”½æœªæ¥ tokenï¼Œè‡ªå›å½’ç”Ÿæˆ
- **Cross-Attention**ï¼šåˆ©ç”¨ encoder çš„ä¿¡æ¯ï¼Œå®ç°æº-ç›®æ ‡å¯¹é½

------

## 7. å¤æ‚åº¦ä¸ä¼˜åŒ–

- Attention å¤æ‚åº¦ï¼šO(nÂ²)ï¼ˆåºåˆ—é•¿æ—¶ç“¶é¢ˆï¼‰
- ä¼˜åŒ–æ–¹æ³•ï¼š
  - ç¨€ç–æ³¨æ„åŠ›ï¼ˆLongformer, BigBirdï¼‰
  - è¿‘ä¼¼æ³¨æ„åŠ›ï¼ˆLinformer, Performerï¼‰
  - ç¼“å­˜ KVï¼ˆTransformer-XLï¼‰

------

## 8. å¸¸è§åº”ç”¨

- Encoder-only â†’ åˆ†ç±»ã€æ£€ç´¢ã€é—®ç­”ï¼ˆBERTï¼‰
- Decoder-only â†’ æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ï¼ˆGPTï¼‰
- Encoder-Decoder â†’ ç¿»è¯‘ã€æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿï¼ˆT5, åŸå§‹ Transformerï¼‰

------

## 9. é¢è¯•å¿…èƒŒå¥

1. â€œAttention çš„æ ¸å¿ƒå…¬å¼æ˜¯ softmax(QKáµ€ / âˆšd_k) Vï¼Œç”¨äºæ•è·ä»»æ„ä½ç½®é—´çš„ä¾èµ–å…³ç³»ã€‚â€
2. â€œMulti-head æå‡äº†æ¨¡å‹è¡¨è¾¾åŠ›ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„ç‰¹å¾ã€‚â€
3. â€œResidual + LayerNorm ç¡®ä¿è®­ç»ƒæ·±å±‚ç½‘ç»œçš„ç¨³å®šã€‚â€
4. â€œDecoder é€šè¿‡ masked self-attention ä¿è¯ç”Ÿæˆçš„è‡ªå›å½’æ€§è´¨ã€‚â€
5. â€œTransformer æœ€å¤§çš„ä¼˜åŠ¿æ˜¯å¹¶è¡ŒåŒ–è®­ç»ƒï¼Œæœ€å¤§çš„ç“¶é¢ˆæ˜¯ O(nÂ²) çš„æ³¨æ„åŠ›è®¡ç®—ã€‚â€

------

âš¡ å»ºè®®ï¼šä½ å¯ä»¥åœ¨çº¸ä¸Šæ‰‹ç”»ä¸€ä¸ª **Transformer Encoder/Decoder æ¡†å›¾**ï¼ˆè¾“å…¥ â†’ Attention â†’ FFN â†’ è¾“å‡ºï¼‰ï¼Œç»“åˆè¿™ä»½å¡ç‰‡èƒŒï¼Œå°è±¡æ›´æ·±ã€‚

------





éå¸¸å¥½çš„é—®é¢˜ï¼Œè¿™ä¸ªæ˜¯ **Transformer é¢è¯•é«˜é¢‘è€ƒç‚¹**ã€‚æˆ‘ç»™ä½ åšä¸€ä¸ª **è¶…è¯¦ç»†å¯¹æ¯”ç¬”è®°**ï¼Œä½ èƒ½ç›´æ¥èƒŒè¯µã€ç°åœºç”»å›¾è®²è§£ã€‚

------

# ğŸ“Œ Transformer çš„ Encoder vs Decoder

## 1. æ€»ä½“ç»“æ„å·®å¼‚

- **Encoder**ï¼šè¾“å…¥åºåˆ— â†’ è¡¨ç¤ºåŒ–ï¼ˆç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡è¡¨ç¤ºï¼‰
- **Decoder**ï¼šæ ¹æ®ç›®æ ‡åºåˆ—ï¼ˆå·²ç”Ÿæˆéƒ¨åˆ†ï¼‰ â†’ é€æ­¥ç”Ÿæˆæ–° token

------

## 2. Encoder ç»“æ„

æ¯å±‚åŒ…å«ï¼š

1. **Multi-Head Self-Attention**ï¼ˆå…¨å¯è§ï¼Œæ²¡æœ‰ maskï¼‰
   - æ¯ä¸ªä½ç½®å¯ä»¥å…³æ³¨æ•´ä¸ªè¾“å…¥åºåˆ—ï¼ˆå…¨å±€ä¸Šä¸‹æ–‡ï¼‰
2. **Add & LayerNorm**ï¼ˆç¨³å®šè®­ç»ƒï¼‰
3. **Feed Forward Network (FFN)**
4. **Add & LayerNorm**

**ç›®çš„**ï¼š

- ç¼–ç è¾“å…¥åºåˆ—ï¼ˆsourceï¼‰ä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œæ•æ‰å…¨å±€ä¾èµ–ã€‚
- ä¾‹å¦‚ï¼šç¿»è¯‘é‡Œï¼ŒæŠŠè‹±æ–‡å¥å­ç¼–ç æˆä¸€ç»„ä¸Šä¸‹æ–‡å‘é‡ã€‚

------

## 3. Decoder ç»“æ„

æ¯å±‚åŒ…å«ï¼š

1. **Masked Multi-Head Self-Attention**
   - åªå…è®¸çœ‹åˆ°å½“å‰å’Œè¿‡å» tokenï¼Œå±è”½æœªæ¥ï¼Œä¿è¯è‡ªå›å½’ç”Ÿæˆã€‚
2. **Add & LayerNorm**
3. **Cross-Attentionï¼ˆEncoder-Decoder Attentionï¼‰**
   - Query æ¥è‡ª decoderï¼ŒKey/Value æ¥è‡ª encoder çš„è¾“å‡º
   - è®© decoder åœ¨ç”Ÿæˆæ—¶å‚è€ƒæºåºåˆ—ï¼ˆå¯¹é½ source å’Œ targetï¼‰
4. **Add & LayerNorm**
5. **Feed Forward Network (FFN)**
6. **Add & LayerNorm**

**ç›®çš„**ï¼š

- æŒ‰é¡ºåºç”Ÿæˆç›®æ ‡åºåˆ—ï¼ˆtargetï¼‰
- æ¯æ¬¡ç”Ÿæˆæ—¶æ—¢èƒ½åˆ©ç”¨å·²ç”Ÿæˆçš„å†…å®¹ï¼ˆmasked self-attnï¼‰ï¼Œä¹Ÿèƒ½å‚è€ƒæºåºåˆ—çš„ä¸Šä¸‹æ–‡ï¼ˆcross-attnï¼‰

------

## 4. æ ¸å¿ƒä¸åŒç‚¹æ€»ç»“

| æ¨¡å—            | Encoder             | Decoder              | ç›®çš„               |
| --------------- | ------------------- | -------------------- | ------------------ |
| Self-Attention  | å…¨åºåˆ—å¯è§          | Maskedï¼ˆä¸èƒ½çœ‹æœªæ¥ï¼‰ | ä¿è¯è‡ªå›å½’ç”Ÿæˆ     |
| Cross-Attention | âŒ æ—                 | âœ… æœ‰                 | åˆ©ç”¨æºåºåˆ—ä¸Šä¸‹æ–‡   |
| è¾“å‡º            | è¡¨ç¤ºå‘é‡ï¼ˆcontextï¼‰ | é€æ­¥é¢„æµ‹ token       | åˆ†åˆ«ç”¨äºç†è§£å’Œç”Ÿæˆ |

------

## 5. ä¸ºä»€ä¹ˆè¿™æ ·æœ‰æ•ˆï¼Ÿ

- **Masked Self-Attention**

  - ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹ç¬¦åˆè¯­è¨€å»ºæ¨¡çš„æ¦‚ç‡åˆ†è§£ï¼š

    P(y)=âˆiP(yiâˆ£y<i,x)P(y) = \prod_i P(y_i \mid y_{<i}, x)

  - æ²¡æœ‰ mask å°±ä¼šâ€œå·çœ‹æœªæ¥â€ï¼Œè®­ç»ƒ/æ¨ç†ä¸ä¸€è‡´ã€‚

- **Cross-Attention**

  - å®ç° â€œå¯¹é½ï¼ˆalignmentï¼‰â€ï¼šdecoder ç”Ÿæˆæ—¶èƒ½åŠ¨æ€å…³æ³¨ encoder çš„ç›¸å…³éƒ¨åˆ†ã€‚
  - æ¯”ä¼ ç»Ÿ seq2seqï¼ˆRNN encoder-decoder + æ³¨æ„åŠ›ï¼‰æ›´å¼ºå¤§ï¼Œå› ä¸ºå¤šå¤´æ³¨æ„åŠ›èƒ½å­¦ä¹ å¤šç§å¯¹é½å…³ç³»ï¼ˆè¯­æ³•ã€è¯­ä¹‰ï¼‰ã€‚

- **Encoder å…¨å±€æ³¨æ„åŠ›**

  - ä¿è¯è¾“å…¥åºåˆ—çš„æ¯ä¸ªä½ç½®éƒ½èƒ½å’Œå…¶ä»–ä½ç½®äº¤äº’ â†’ é•¿è·ç¦»ä¾èµ–é—®é¢˜è¢«è§£å†³ã€‚

------

## 6. ä¸€å¥è¯æ€»ç»“ï¼ˆé¢è¯•ç­”æ³•ï¼‰

1. â€œEncoder çš„ä½œç”¨æ˜¯å¯¹è¾“å…¥åºåˆ—è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œè¾“å‡ºè¯­ä¹‰è¡¨ç¤ºï¼›Decoder åˆ™åœ¨è‡ªå›å½’ç”Ÿæˆçš„åŒæ—¶ï¼Œé€šè¿‡ cross-attention åŠ¨æ€åˆ©ç”¨ encoder çš„è¡¨ç¤ºã€‚â€
2. â€œEncoder ç”¨å…¨å±€ self-attention æ•è·è¾“å…¥çš„ä¾èµ–ï¼ŒDecoder ç”¨ masked self-attention ä¿è¯å› æœæ€§ï¼Œå†ç”¨ cross-attention å’Œæºåºåˆ—å¯¹é½ã€‚â€
3. â€œè¿™ç§ç»“æ„æœ‰æ•ˆçš„åŸå› æ˜¯ï¼šEncoder-Decoder åˆ†å·¥æ˜ç¡®ï¼Œä¸€ä¸ªè´Ÿè´£ç†è§£è¾“å…¥ï¼Œä¸€ä¸ªè´Ÿè´£æ¡ä»¶ç”Ÿæˆï¼Œé¿å…äº†ä¿¡æ¯æ³„éœ²ï¼ŒåŒæ—¶åˆ©ç”¨äº†å…¨å±€ä¾èµ–å’Œå¯¹é½æœºåˆ¶ã€‚â€

------

## 7. å›¾ç¤ºï¼ˆå»ºè®®ä½ é¢è¯•æ—¶æ‰‹ç”»ï¼‰

```
Encoder: [è¾“å…¥åºåˆ—] â†’ [Self-Attention(å…¨å±€)] â†’ [FFN] â†’ [ç¼–ç è¡¨ç¤º]

Decoder: [å·²ç”Ÿæˆåºåˆ—] â†’ [Masked Self-Attention] 
                          â†“
         [Encoderè¾“å‡º] â†’ [Cross-Attention] â†’ [FFN] â†’ [è¾“å‡ºæ–°token]
```

------

âœ… èƒŒè¯µé‡ç‚¹ï¼š

- Encoderï¼šå…¨å±€ self-attnï¼Œæ—  cross-attnï¼Œç›®çš„æ˜¯ç†è§£è¾“å…¥ã€‚
- Decoderï¼šmasked self-attn + cross-attnï¼Œç›®çš„æ˜¯æ¡ä»¶ç”Ÿæˆã€‚
- æœ‰æ•ˆæ€§ï¼šé¿å…ä¿¡æ¯æ³„éœ²ï¼Œä¿è¯è‡ªå›å½’ï¼Œå¼ºåŒ–è¾“å…¥è¾“å‡ºå¯¹é½ã€‚

---





å¥½é—®é¢˜ ğŸ‘ï¼Œ**LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰** æ˜¯ Transformer ä¸­ä¸€ä¸ªéå¸¸å…³é”®çš„æ¨¡å—ï¼Œç»å¸¸å‡ºç°åœ¨ **Attention/FFN ä¹‹åçš„æ®‹å·®ç»“æ„é‡Œ**ã€‚

------

# ğŸ“Œ LayerNormï¼ˆLayer Normalizationï¼‰

## 1. å®šä¹‰

LayerNorm æ˜¯ä¸€ç§ **å½’ä¸€åŒ–æ–¹æ³•**ï¼Œå®ƒå’Œ BatchNorm ç±»ä¼¼ï¼Œä½†å½’ä¸€åŒ–çš„ç»´åº¦ä¸åŒã€‚

- **BatchNorm**ï¼šåœ¨ **batch ç»´åº¦** ä¸Šåšå½’ä¸€åŒ–ï¼ˆè·¨æ ·æœ¬ï¼‰
- **LayerNorm**ï¼šåœ¨ **ç‰¹å¾ç»´åº¦** ä¸Šåšå½’ä¸€åŒ–ï¼ˆæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ï¼‰

å…¬å¼ï¼š

LayerNorm(x)=xâˆ’Î¼Ïƒâ‹…Î³+Î²\text{LayerNorm}(x) = \frac{x - \mu}{\sigma} \cdot \gamma + \beta

å…¶ä¸­ï¼š

- xxï¼šè¾“å…¥å‘é‡ï¼ˆæ¯”å¦‚ d_model ç»´åº¦ï¼‰
- Î¼\muï¼šå½“å‰æ ·æœ¬çš„å‡å€¼
- Ïƒ\sigmaï¼šå½“å‰æ ·æœ¬çš„æ ‡å‡†å·®
- Î³,Î²\gamma, \betaï¼šå¯å­¦ä¹ å‚æ•°ï¼ˆç¼©æ”¾å’Œå¹³ç§»ï¼‰

------

## 2. ä½œç”¨

1. **ç¨³å®šè®­ç»ƒ**
   - é¿å…ä¸åŒ token è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´å·®å¼‚è¿‡å¤§ï¼Œè®­ç»ƒæ›´å¹³ç¨³ã€‚
2. **è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**
   - ä¿æŒè¾“å…¥è¾“å‡ºåˆ†å¸ƒç¨³å®šï¼Œæ·±å±‚ç½‘ç»œæ›´å®¹æ˜“æ”¶æ•›ã€‚
3. **å’Œæ®‹å·®ç»“åˆ**
   - Transformer çš„æ¯ä¸ªå­å±‚ï¼ˆAttentionã€FFNï¼‰åéƒ½åŠ æ®‹å·® + LayerNormï¼Œæé«˜æ¢¯åº¦æµåŠ¨æ€§ã€‚

------

## 3. ä¸ºä»€ä¹ˆä¸ç”¨ BatchNormï¼Ÿ

- Transformer çš„è¾“å…¥æ˜¯åºåˆ—ï¼ˆé•¿åº¦å¯å˜ï¼‰ï¼Œè€Œä¸”åœ¨ NLP é‡Œ **batch çš„å¤§å°å’Œåˆ†å¸ƒä¸ç¨³å®š**ã€‚
- **BatchNorm å¯¹ batch size æ•æ„Ÿ**ï¼ˆå° batch è®­ç»ƒæ•ˆæœå·®ï¼‰ï¼Œè€Œ **LayerNorm ç‹¬ç«‹å¤„ç†æ¯ä¸ªæ ·æœ¬**ï¼Œæ›´é€‚åˆåºåˆ—å»ºæ¨¡ã€‚

------

## 4. å®é™…ä½ç½®

æœ‰ä¸¤ç§å¸¸è§å†™æ³•ï¼š

- **Post-Norm**ï¼ˆåŸå§‹è®ºæ–‡ï¼‰ï¼šSublayer(x) + x â†’ LayerNorm
- **Pre-Norm**ï¼ˆåæ¥æ›´å¸¸ç”¨ï¼‰ï¼šå…ˆ LayerNormï¼Œå†è¿› Sublayer â†’ å†æ®‹å·®
   ğŸ‘‰ Pre-Norm åœ¨æ·±å±‚ Transformer ä¸­æ›´ç¨³å®šï¼ˆå‡å°‘æ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚

------

## 5. PyTorch ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn as nn

# è¾“å…¥: batch=2, åºåˆ—é•¿=3, éšå±‚ç»´åº¦=4
x = torch.randn(2, 3, 4)

layernorm = nn.LayerNorm(normalized_shape=4)  # å¯¹æœ€åä¸€ç»´åšå½’ä¸€åŒ–
y = layernorm(x)

print("è¾“å…¥:", x)
print("è¾“å‡º:", y)
```

------

âœ… **ä¸€å¥è¯æ€»ç»“ï¼ˆé¢è¯•èƒŒè¯µç‰ˆï¼‰ï¼š**
 â€œLayerNorm æ˜¯åœ¨ç‰¹å¾ç»´åº¦ä¸Šåšå½’ä¸€åŒ–ï¼Œæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å½’ä¸€åŒ–ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šå’Œæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„é—®é¢˜ï¼Œæ¯” BatchNorm æ›´é€‚åˆ NLP é‡Œçš„ Transformer ç»“æ„ã€‚â€

------







---



**æ®‹å·®ç½‘ç»œï¼ˆResidual Network, ResNetï¼‰** æ˜¯æ·±åº¦å­¦ä¹ é‡Œéå¸¸æ ¸å¿ƒçš„æŠ€å·§ï¼ŒTransformer ä¹Ÿå¤§é‡ä½¿ç”¨ã€‚ä¸‹é¢ç»™ä½ åšä¸€ä»½é¢è¯•å¯èƒŒçš„é€Ÿè®°ç¬”è®°ã€‚

------

# ğŸ“Œ æ®‹å·®ç½‘ç»œï¼ˆResidual Connectionï¼‰

## 1. å®šä¹‰

æ®‹å·®ç½‘ç»œåœ¨å­å±‚çš„è¾“å‡ºå¤–é¢åŠ ä¸€ä¸ªâ€œæ·å¾„è¿æ¥â€ï¼ˆshortcut connectionï¼‰ï¼š

y=F(x)+xy = F(x) + x

å…¶ä¸­ï¼š

- xxï¼šè¾“å…¥
- F(x)F(x)ï¼šå­å±‚ï¼ˆå¦‚ Attention æˆ– FFNï¼‰çš„è¾“å‡º
- yyï¼šæ®‹å·®åçš„è¾“å‡º

------

## 2. ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®ï¼Ÿ

### (1) è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

- åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦å¯èƒ½åœ¨ä¼ æ’­æ—¶æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾ã€‚
- æ®‹å·®æä¾›äº†ä¸€æ¡â€œç›´æ¥è·¯å¾„â€ï¼Œæ¢¯åº¦èƒ½ç›´æ¥ä¼ é€’å›å»ã€‚

### (2) ç¼“è§£é€€åŒ–é—®é¢˜

- éšç€ç½‘ç»œåŠ æ·±ï¼Œè®­ç»ƒè¯¯å·®å¯èƒ½ä¸é™åå‡ï¼ˆé€€åŒ–ç°è±¡ï¼‰ã€‚
- æ®‹å·®è®©æ·±å±‚ç½‘ç»œè‡³å°‘å¯ä»¥å­¦åˆ°â€œæ’ç­‰æ˜ å°„â€ï¼ˆå³ F(x)=0ï¼Œè¾“å‡º=è¾“å…¥ï¼‰ï¼Œä¸ä¼šæ¯”æµ…å±‚æ›´å·®ã€‚

### (3) æå‡è®­ç»ƒé€Ÿåº¦ä¸æ€§èƒ½

- æ®‹å·®ä½¿ä¼˜åŒ–æ›´å®¹æ˜“ï¼Œæ·±å±‚ç½‘ç»œæ›´å®¹æ˜“æ”¶æ•›ã€‚
- å®éªŒè¯æ˜ï¼Œæ®‹å·®ç»“æ„èƒ½è®­ç»ƒä¸Šç™¾å±‚ç”šè‡³ä¸Šåƒå±‚ç½‘ç»œï¼Œè€Œæ²¡æœ‰ä¸¥é‡é€€åŒ–ã€‚

------

## 3. åœ¨ Transformer ä¸­çš„åº”ç”¨

- **ä½ç½®**ï¼šæ¯ä¸ªå­å±‚ï¼ˆSelf-Attention / FFNï¼‰åé¢éƒ½åŠ æ®‹å·®ï¼Œå†åš LayerNormï¼š

  output=LayerNorm(x+Sublayer(x))\text{output} = \text{LayerNorm}(x + \text{Sublayer}(x))

- **ç›®çš„**ï¼š

  - ä¿è¯æ·±å±‚ Transformerï¼ˆå‡ åç”šè‡³ä¸Šç™¾å±‚ï¼‰ä»ç„¶å¯è®­ç»ƒ
  - æå‡æ¢¯åº¦æµåŠ¨æ€§
  - é¿å…æ¨¡å‹é™·å…¥è®­ç»ƒåœæ»

------

## 4. å’Œ LayerNorm çš„å…³ç³»

- æ®‹å·®è¿æ¥ä¿è¯äº†æ¢¯åº¦æµç•…
- LayerNorm ä¿è¯æ•°å€¼ç¨³å®šã€åˆ†å¸ƒä¸å‘æ•£
- ä¸¤è€…ç»„åˆ = **ç¨³å®š + é«˜æ•ˆçš„æ·±å±‚è®­ç»ƒ**

------

## 5. ä¸€å¥è¯æ€»ç»“ï¼ˆé¢è¯•ç­”æ³•ï¼‰

â€œæ®‹å·®ç½‘ç»œçš„ä½œç”¨æ˜¯ç»™æ·±å±‚æ¨¡å‹æä¾›ä¸€æ¡æ·å¾„ï¼Œè®©æ¨¡å‹æ›´å®¹æ˜“è®­ç»ƒï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œé€€åŒ–é—®é¢˜ã€‚åœ¨ Transformer é‡Œï¼Œæ¯ä¸ªå­å±‚éƒ½ç”¨æ®‹å·® + LayerNormï¼Œè¿™ä¿è¯äº†å‡ åå±‚ç”šè‡³ä¸Šç™¾å±‚çš„ Transformer èƒ½ç¨³å®šæ”¶æ•›ã€‚â€

