# HuggingFace 常用微调技术一览（含代码示例，中文注释）

> 目标：用 HuggingFace 生态（`transformers` + `peft` + `trl` + `bitsandbytes`）快速跑通**常用微调技术**，包含：
>
> - 全参数微调（Full FT）
> - 部分冻结/分层微调（Layer-wise/Partial FT）
> - BitFit（仅 bias）
> - Adapter（Houlsby/Pfeiffer）
> - LoRA / AdaLoRA / IA³
> - Prefix Tuning / Prompt Tuning / P-Tuning v2
> - QLoRA（4bit 量化 + LoRA）
> - 指令微调（SFT）与偏好对齐（DPO）
>
> 每节给出**最小可改造代码**；示例以 Causal LM（聊天/生成）为主，BERT 类任务可类比替换 `AutoModelForSequenceClassification` 等。

------

## 0. 统一准备（数据与 Tokenizer）

```python
# common_prep.py —— 统一的数据与分词器准备（指令微调风格）
from datasets import load_dataset
from transformers import AutoTokenizer

MODEL_ID = "Qwen/Qwen2-1.5B-Instruct"  # 可替换任意 HF 模型

# 简单的指令数据：{"instruction","input","output"}
PROMPT = (
    "### 指令:\n{instruction}\n\n"
    "### 输入:\n{input}\n\n"
    "### 回答:\n"
)

def build_tokenizer(model_id=MODEL_ID):
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    tok.padding_side = "right"
    return tok


def format_fn(example, tokenizer):
    inst = example.get("instruction", "").strip()
    inp  = example.get("input", "").strip() or "无"
    out  = example.get("output", "").strip()
    prompt = PROMPT.format(instruction=inst, input=inp)
    full = prompt + out
    model_in = tokenizer(full, truncation=True)

    # 标签遮盖：prompt 区域 -100，回答区域正常
    p_ids = tokenizer(prompt, add_special_tokens=False)["input_ids"]
    labels = [-100]*len(p_ids) + model_in["input_ids"][len(p_ids):]
    model_in["labels"] = labels[: tokenizer.model_max_length]
    return model_in


def load_sft_dataset(tokenizer):
    ds_train = load_dataset("json", data_files="data/train.jsonl")["train"]
    ds_val   = load_dataset("json", data_files="data/valid.jsonl")["train"]
    ds_train = ds_train.map(lambda x: format_fn(x, tokenizer), remove_columns=ds_train.column_names)
    ds_val   = ds_val.map(lambda x: format_fn(x, tokenizer), remove_columns=ds_val.column_names)
    return ds_train, ds_val
```

------

## 1) 全参数微调（Full Fine-Tuning）

> 直接训练所有参数；效果好、代价高，需较大显存与数据。

```python
# 01_full_ft.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
tokenizer = build_tokenizer(MODEL_ID)
train_ds, val_ds = load_sft_dataset(tokenizer)

args = TrainingArguments(
    output_dir="outputs/full_ft",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    learning_rate=5e-5,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_steps=200,
    bf16=True, fp16=False,
    report_to=["none"],
)

trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds)
trainer.train()
trainer.save_model("outputs/full_ft")
```

------

## 2) 分层/部分微调（冻结底层，只训顶层）

> 冻结词嵌入/前 N 层，仅训练上层，降低显存与过拟合风险。

```python
# 02_partial_ft.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)

# 例：冻结前 12 层（名称因模型而异，需 print(model) 查找层名）
for n, p in model.named_parameters():
    if any(x in n for x in ["embed_tokens", "model.layers.0", "model.layers.1", "model.layers.2", "model.layers.3",
                             "model.layers.4", "model.layers.5", "model.layers.6", "model.layers.7", "model.layers.8",
                             "model.layers.9", "model.layers.10", "model.layers.11"]):
        p.requires_grad = False

# 其余保持训练

trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="outputs/partial_ft",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        num_train_epochs=1,
        learning_rate=1e-4,
        logging_steps=10,
        bf16=True,
        report_to=["none"],
    ),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
trainer.save_model("outputs/partial_ft")
```

------

## 3) BitFit（只训练 bias）

> 极简参数高效微调：只更新偏置项 `bias`，适合小数据/资源紧张。

```python
# 03_bitfit.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
for n, p in model.named_parameters():
    p.requires_grad = ("bias" in n)

trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="outputs/bitfit",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=1e-3,  # BitFit 通常可用稍大学习率
        num_train_epochs=1,
        bf16=True,
        logging_steps=10,
        report_to=["none"],
    ),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
trainer.save_model("outputs/bitfit")
```

------

## 4) Adapter（Houlsby/Pfeiffer）

> 在每层插入小瓶颈模块，仅训练 Adapter 参数；`peft` 支持。

```python
# 04_adapter.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import get_peft_model, LoraConfig, TaskType, AdaLoraConfig, PeftType, PromptTuningConfig, PrefixTuningConfig
from peft import (
    get_peft_model_state_dict, PeftModel, PromptEncoderConfig, AdaptionPromptConfig,
)
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

# Note: 经典 Adapter(Houlsby/Pfeiffer) 在 peft 中走的是 "ADAPTER" 路线（某些版本作为 beta），
# 下例演示通用接口；若你的 peft 版本缺省，可改用 LoRA 等稳定方法。

from peft import AdaptionPromptConfig  # 作为适配示例（与 Adapter 类似按层插入）

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
peft_cfg = AdaptionPromptConfig(
    task_type=TaskType.CAUSAL_LM,
    adapter_layers=16,     # 插入层数（示例）
    adapter_len=64,        # 瓶颈长度
)
model = get_peft_model(model, peft_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/adapter", per_device_train_batch_size=2, num_train_epochs=1, bf16=True, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/adapter")
```

> 注：传统 Houlsby/Pfeiffer Adapter 在不同 peft 版本暴露方式略有差异；若遇到接口不稳定，推荐 **LoRA** 作为更通用替代。

------

## 5) LoRA（最常用）

> 在大权重矩阵上添加低秩分解增量（只训练 A、B），显存友好、效果稳。

```python
# 05_lora.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
lora_cfg = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type=TaskType.CAUSAL_LM,
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

args = TrainingArguments(
    output_dir="outputs/lora",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=1,
    bf16=True,
    logging_steps=10,
    report_to=["none"],
)

trainer = Trainer(model=model, args=args, train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0])
trainer.train()
model.save_pretrained("outputs/lora")
```

------

## 6) AdaLoRA（自适应低秩）

> 训练过程中**动态分配秩**，提升参数利用率。

```python
# 06_adalora.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import AdaLoraConfig, get_peft_model, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
adalora_cfg = AdaLoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16, lora_alpha=32, target_r=8, beta1=0.85, beta2=0.85, tinit=200, tfinal=1000,
)
model = get_peft_model(model, adalora_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/adalora", per_device_train_batch_size=2, num_train_epochs=1, bf16=True, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/adalora")
```

------

## 7) IA³（乘性缩放）

> 只在注意力/FFN 的**通道维**引入可训练缩放向量，参数极少。

```python
# 07_ia3.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import IA3Config, get_peft_model, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
ia3_cfg = IA3Config(task_type=TaskType.CAUSAL_LM)
model = get_peft_model(model, ia3_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/ia3", per_device_train_batch_size=2, num_train_epochs=1, bf16=True, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/ia3")
```

------

## 8) Prefix Tuning（前缀虚拟向量）

> 为每层注意力添加可训练的**前缀键值**，冻结原模型权重。

```python
# 08_prefix_tuning.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import PrefixTuningConfig, get_peft_model, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
prefix_cfg = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=32)
model = get_peft_model(model, prefix_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/prefix", per_device_train_batch_size=2, num_train_epochs=1, bf16=True, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/prefix")
```

------

## 9) Prompt Tuning（可训练的词嵌入提示）

> 在输入侧拼接若干**可训练提示向量**，不改模型内部结构。

```python
# 09_prompt_tuning.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import PromptTuningConfig, get_peft_model, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
prompt_cfg = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, prompt_tuning_init="TEXT", num_virtual_tokens=20,
                                prompt_tuning_init_text="你是 helpful 的中文助手。")
model = get_peft_model(model, prompt_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/prompt", per_device_train_batch_size=2, num_train_epochs=1, bf16=True, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/prompt")
```

------

## 10) P-Tuning v2（Prompt Encoder）

> 用小型 MLP/LSTM 对虚拟提示做**深层注入**，效果通常优于浅层 Prompt Tuning。

```python
# 10_ptuning_v2.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import PromptEncoderConfig, get_peft_model, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
pt_cfg = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=32, encoder_hidden_size=256)
model = get_peft_model(model, pt_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/ptuning", per_device_train_batch_size=2, num_train_epochs=1, bf16=True, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/ptuning")
```

------

## 11) QLoRA（4bit 量化 + LoRA）

> 最火的参数高效微调路线：**显存极省**，性能接近 LoRA 全精度。

```python
# 11_qlora.py
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from common_prep import build_tokenizer, load_sft_dataset, MODEL_ID

# 4bit 量化加载
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=None,
    device_map="auto",
    trust_remote_code=True,
)
model = prepare_model_for_kbit_training(model)

lora_cfg = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, task_type=TaskType.CAUSAL_LM, bias="none")
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="outputs/qlora", per_device_train_batch_size=2, num_train_epochs=1, learning_rate=2e-4,
                           gradient_accumulation_steps=8, bf16=True, logging_steps=10, report_to=["none"]),
    train_dataset=load_sft_dataset(build_tokenizer(MODEL_ID))[0],
)
trainer.train()
model.save_pretrained("outputs/qlora")
```

------

## 12) DPO（偏好优化，对齐）

> 使用 TRL 库实现 Direct Preference Optimization（不需要训练 reward model）。

```python
# 12_dpo.py  —— 需要准备偏好数据：同一个 prompt 的 (chosen, rejected)
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM
from datasets import load_dataset
from common_prep import build_tokenizer, MODEL_ID

# 数据示例结构：{"prompt":..., "chosen":..., "rejected":...}
dataset = load_dataset("json", data_files="data/dpo.jsonl")

model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype="auto", device_map="auto", trust_remote_code=True)
tokenizer = build_tokenizer(MODEL_ID)

cfg = DPOConfig(
    output_dir="outputs/dpo",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=5e-6,
    num_train_epochs=1,
    bf16=True,
)

trainer = DPOTrainer(
    model=model,
    args=cfg,
    beta=0.1,  # 重要超参：偏好强度
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)
trainer.train()
trainer.save_model("outputs/dpo")
```

------

## 13) 合并与导出（以 LoRA/QLoRA 为例）

```python
# 13_merge_lora.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch, os

BASE = MODEL_ID
ADAPTER = "outputs/qlora"
MERGED = "outputs/merged"
os.makedirs(MERGED, exist_ok=True)

base = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
peft_model = PeftModel.from_pretrained(base, ADAPTER)
merged = peft_model.merge_and_unload()  # 合并 LoRA 权重
merged.save_pretrained(MERGED)
AutoTokenizer.from_pretrained(BASE).save_pretrained(MERGED)
```

------

## 14) 选择建议与调参口诀

- **显存紧张**：QLoRA ＞ LoRA ≈ IA³ ＞ Prefix/Prompt ＞ BitFit ＞ Partial FT ＞ Full FT（从省到费）
- **数据很少**：Prompt/Prefix、IA³、BitFit、LoRA；学习率可稍大（1e-3 ~ 2e-4）。
- **质量优先**：LoRA/Full FT；底层冻结 + 顶层微调作为折中。
- **对齐偏好**：先 SFT，再 DPO/ORPO/KTO（TRL）。
- **部署**：合并权重或用 vLLM `--adapter`；注意 `max-model-len` 与显存平衡。

------

### 参考目录结构

```text
project/
├─ data/
│  ├─ train.jsonl
│  ├─ valid.jsonl
│  └─ dpo.jsonl
├─ outputs/
│  ├─ full_ft/ partial_ft/ bitfit/ lora/ adalora/ ia3/ prefix/ prompt/ ptuning/ qlora/ dpo/
│  └─ merged/
├─ common_prep.py
├─ 01_full_ft.py ... 13_merge_lora.py
└─ HF_Finetuning_Techniques_With_Code_Chinese.md  # 本文档
```

> 如果你告诉我**GPU/显存**与**具体模型**，我可以把上述脚本里的 batch、accum、precision、LoRA 超参替你改成**百分百可跑**的配置。